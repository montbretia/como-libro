// vim: spell spelllang=es
#import "../utils.typ": question, aside
#set enum(numbering: "1)")

= Ciencia cognitiva y saber cómo

En los capítulos anteriores hemos examinado lo que puede considerarse como el
tema central de las discusiones filosóficas acerca del saber-cómo: la disputa
entre el intelectualismo y el anti-intelectualismo. En este punto deberíamos ser
capaces de tomar posición en esa disputa, o al menos tener una idea acerca de lo
que implicaría adoptar una posición. Sin embargo, esto no agota el tema del
saber cómo; hay más problemas interesantes que vale la pena tratar con cierto
detalle, independientemente de qué postura tome uno respecto al
intelectualismo/anti-intelectualismo. En este capítulo y los siguientes vamos a
ver algunos de ellos.

Primero: sea cual sea nuestra posición respecto al debate entre intelectualistas
y anti-intelectualistas, el saber cómo es un fenómeno en parte psicológico y
cognitivo. Muchos filósofos en la actualidad suscriben a lo que llamaríamos un
_naturalismo mínimo_, que consiste en la idea que la teorización filosófica debe
ser al menos compatible con los métodos y resultados de la ciencia. Una manera
de satisfacer esta condición es requerir además que el trabajo filosófico esté
informado científicamente—esta es una forma más sustantiva de naturalismo. En el
caso del saber cómo, desde una perspectiva naturalista como esta, va a ser
importante examinar qué dicen las ciencias acerca de la naturaleza de los
estados mentales asociados al saber cómo. En este capítulo vamos a examinar
brevemente ciertas líneas de investigación psicológica que son relevantes. El
primer problema que vamos a examinar es cuál podría ser el formato
representacional asociado al saber cómo. Suponiendo que el saber cómo requiere
poseer cierta información acerca de cómo realizar ciertas tareas, ¿cómo son
representadas estas tareas desde una perspectiva representacionalista? Por
supuesto, esto asume que el modelo representacionalista de la mente es correcto.
A partir de esta misma suposición, podríamos inferir que podría haber cierta
clase de agentes artificiales que poseyesen saber cómo—robots con conocimiento
práctico. ¿Es plausible esto?

== Representación mental

No es una exageración decir que la ciencia cognitiva moderna tradicional está
fundada sobre la noción de _representación mental_. En cierto sentido, la noción
general de representación es cotidiana: una pintura puede representar algo, así
como puede hacerlo una palabra, etc. La idea de representación mental es la idea
de que existen ítemes mentales que tienen la propiedad de representar algo. Una
manera de poner la cuestión es que hay estados mentales que son acerca de algo.
Cuando imagino a mi perra Ruby esperándome junto a la puerta, estoy en un estado
mental que apunta a Ruby, es decir, que es acerca de ella. Cuando sumo los
números 2 y 3, estoy en un estado mental acerca de esos números.

De esta observación a menudo se ha pasado a la idea de que nuestros estados
mentales no lo son acerca de algo, sino que "cargan", por así decirlo, con
información acerca de aquello de lo que tratan--es decir, que algunos de
nuestros estados mentales tienen _contenido_. Esto también proviene de la idea
genérica de representación: una pintura, por ejemplo, podría no ser solamente
acerca de algún sujeto, sino además representarlo de una cierta manera. Una
representación representa algo "de una manera" en dos sentidos. Por un lado,
distintas pinturas de un mismo sujeto lo representan de distinta manera porque
tienen distinto contenido. Si veo a mi madre sentada en el sofá desde la
cocina, la veo de distinta manera que cuando la miro desde el comedor; la
diferencia está en el contenido de mi estado mental. En otros casos,
representamos un mismo objeto con un mismo contenido, pero de distintas maneras
en el sentido de que el _modo de representación_ es distinto. Por ejemplo, si
estoy seguro de que mañana no va a llover, tengo un estado mental que representa
la situación posible de que mañana no lloverá. Pero si me pregunto si mañana no
lloverá, también tengo un estado mental que representa esa situación, aunque de
otro modo. Ya vimos esto cuando vimos cómo los intelectualistas acerca del
saber-cómo a veces apelan a la idea de que el saber-cómo requiere un modo
"práctico" de representación, que lo distingue de otros estados mentales con el
mismo contenido proposicional.

Es importante tener en mente que el carácter representacional de las
representaciones, lo que las hace representaciones como tales, es algo
extrínseco, que depende de la existencia de sujetos que puedan reconocerlas como
representaciones. Todo tipo de cosas puede _proveer_ de información que alguien
puede reconocer como acerca de algo. Los grupos de letras que lees ahora
representan palabras, que representan cosas. Para que sirvan ese propósito es
necesario que las reconozcas como representaciones--es necesario que tengas una
manera de _interpretarlas_ (en su sentido más general, tener la capacidad de
interpretar algo como representación consiste en tener la disposición de actuar
de cierta manera cuando uno lo reconoce).

Esto, a su vez, nos da una manera de entender el rol que pueden cumplir las
representaciones mentales. Al reconocer una representación, la interpretamos
realizando una acción. De este modo, las representaciones pueden cumplir el rol
de proveer de información que puede ser empleada luego para guiar la acción.
Vemos que un auto viene por la calle; las representaciones que formamos del auto
nos sirven para _estimar_ la velocidad del vehículo, y la distancia a la que está
de nosotros. En consecuencia, _juzgamos_ que podemos o no tratar de cruzar la
calle. Este ejemplo ilustra además que las acciones que podemos tomar en base a
las representaciones que formamos pueden consistir también en formar otras
representaciones.

Esto da pie a la siguiente imagen sobre la arquitectura de la mente (la manera
en que la mente está organizada funcionalmente). Por un lado, tenemos una
interfaz entre el mundo y la mente, que normalmente consiste en un conjunto de
dispositivos que adquieren información del sujeto mismo y su entorno y forman
representaciones. Luego, tenemos un sistema que procesa esa información de
distintas maneras y produce otras representaciones. Finalmente, hay otra
interfaz entre la mente y el mundo, que consiste en una serie de mecanismos
mediante los cuales el organismo realiza acciones en el mundo. En este modelo,
la mente simplemente es un sistema que transforma información disponible en
información utilizable.

Una versión clásica de esta idea es lo que podemos llamar _la teoría
computacional de la mente_.#footnote[Estrictamente hablando, la teoría
computacional no requiere que tengamos que adoptar el concepto de representación
mental (por ejemplo, ciertos modelos "conexionistas" rechazan la idea de
representación mental--aunque es contencioso si en realidad escapan del modelo
representacionalista), pero sus versiones más tradicionales sí combinan estas
ideas.] De acuerdo a esta, las operaciones mentales consisten en procesos
computacionales; la mente funciona de una manera equivalente a la de una
computadora simbólica.

Una de las maneras más generales de entender cómo funciona una computadora es la
que desarrolló Alan Turing en un paper en 1937. Según Turing, es posible
describir cualquier proceso computacional en términos de la operación de un
cierto tipo de máquina extremadamente simple; a estas máquinas se las llama
_máquinas de Turing_. Una máquina de Turing consiste en: 

+ una cinta de longitud arbitrariamente larga, dividida en secciones, en la cual
  pueden escribirse símbolos de un lenguaje determinado,
+ un cabezal lector-escritor con la capacidad de leer y escribir símbolos en la
  cinta en distintas posiciones,
+ un registro del estado de la máquina,
+ una tabla finita de instrucciones tales que, dado el estado de la máquina y el
  símbolo que se lee bajo el cabezal, la máquina:
  + borra el símbolo bajo el cabezal o lo sobreescribe,
  + mueve el cabezal a la izquierda o la derecha, o mantiene el cabezal en la
    posición actual,
  + asume un nuevo estado.

Una máquina de Turing siempre va a tener un estado inicial, y puede llegar a
detenerse (llegar al estado de _parada_).#footnote[Dado un sistema computacional
con un cierto input, uno podría querer saber si la máquina que lo ejecuta va a
llegar eventualmente a detenerse o no. Este problema, conocido como el _problema
de la parada_, no puede resolverse en general; es decir, no hay un algoritmo que
determine, dados un programa y un input, que el programa eventualmente va a
detenerse.] Por ejemplo, supongamos que tenemos una máquina con una cinta
inicialmente vacía con el cabezal en la posición 0:

#figure(kind: "misc", supplement: none)[#table(rows: 1.5em, columns: (1.5em,) * 10, stroke: 0.1pt)]

Asumamos que el lenguaje solo contiene los símbolos "0" y "1". Queremos producir
la cadena de símbolos "1"--es decir, ejecutar un programa que se detenga después
de emitir esa cadena de símbolos. Los inputs del programa son el estado y el
símbolo bajo el lector. Entonces, el programa puede definirse simplemente con la
regla que dice que, dado el estado inicial $sigma$ y cualquier símbolo bajo el
lector, el cabezal debe escribir el símbolo "1", mantener la posición, y pasar
al estado de parada. Tras una aplicación del programa, la máquina queda en la
siguiente configuración (la posición del cabezal está en rojo):

#figure(kind: "misc", supplement: none)[#table(rows: 1.5em, columns: (1.5em,) * 10, stroke: 0.1pt,
table.cell(fill: red.lighten(50%))[1])]

#question[¿Qué pasaría si reemplazamos esa regla por la regla que dice que, dado
el estado inicial $sigma$ y cualquier símbolo, el cabezal debe escribir el
símbolo "1", moverse a la derecha, y pasar al estado $sigma$ nuevamente?]

Este ejemplo es, quizás, demasiado simple. Imaginemos que quisiéramos producir
la cadena de símbolos "10101010...", con el "10" repitiéndose infinitamente.
¿Qué programa produciría esta cadena de símbolos? En el estado inicial $sigma$,
hacemos que el cabezal escriba "1", nos movemos a la derecha y pasamos al estado
$sigma'$. En el estado $sigma'$, escribimos "0", nos movemos a la derecha, y
pasamos al estado $sigma$ nuevamente.

#figure(caption: [_Algunas iteraciones del programa. La posición del cabezal está
marcada en rojo._])[
    #table(
        rows: 1.5em, 
        columns: (1.5em,) * 12, 
        stroke: 0.1pt,
        // row-gutter: 0.2em,
    table.cell(stroke: none)[1], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [], [], [], [], table.cell(stroke: none)[],
    table.cell(stroke: none)[2], [1], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none)[3], [1], [0], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none)[4], [1], [0], [1], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none, colspan: 12)[...]
)
]

El modelo de las máquinas de Turing puede generalizarse de maneras interesantes
que permiten sugerir una cierta analogía entre los procesos mentales y los
procesos computacionales que el modelo captura. Por ejemplo, en vez de asumir
que el sistema tiene acceso a una única cinta, es posible proveerlo de "cintas"
para el ingreso de información desde alguna fuente externa que opera
independientemente (del mismo modo que los sentidos de un organismo son
afectados por la operación independientemente de cosas externas al organismo).
La información que ingresa al sistema mediante estas cintas puede después ser
procesada por el sistema. Del mismo modo, puede conectarse el sistema a
distintos mecanismos que produzcan todo tipo de efectos (un sistema de
"salida"), del mismo modo que (asumimos) los procesos mentales de un sujeto
pueden manifestarse en su comportamiento explícito.

Una de las características más notables de este modelo de la operación de la
mente es que permite abstraer de los detalles de la manera en que se realizan
las operaciones. Distintas máquinas de Turing pueden computar lo mismo llevando
a cabo secuencias distintas de operaciones; y además, los detalles de cómo se
realicen esas operaciones no son importantes. Un computador moderno funciona de
una manera muy distinta a cómo habría funcionado la máquina analítica de
Babbage, pero ambos pueden en principio realizar los mismos tipos de
procedimientos computacionales. Y si la caracterización computacional de la
mente es plausible, no importa que las partes de nuestro cuerpo que están a
cargo de la ejecución de procesos computacionales sea distinta a las partes de
una computadora digital, ya que ambos pueden en principio instanciar los mismos
programas. De este modo, el modelo computacional sugiere que al menos cierta
clase de procesos mentales pueden replicarse en la operación de procesos
computacionales artificiales.#footnote[Un paper clásico que defiende esta idea
es "Mind and Machines" (1960), de Hillary Putnam. Putnam posteriomente rechazó
su entusiasmo inicial por el computacionalismo mental; véase por ejemplo su
libro _Representation and Reality_ (1988).]

La noción más general de representación se mezcla bien con el computacionalismo
mental porque amplía el alcance de los tipos de procesos que pueden capturarse
por el modelo. Considérese el caso de la clasificación de objetos en una escena
visual: un sujeto percibe su entorno, y llega a determinar que existen tales o
cuáles objetos en este (por ejemplo, ahora juzgo que hay un trozo de chocolate
en mi escritorio en base al contenido de mi experiencia visual). La mente del
sujeto recibe información de su entorno, y lo procesa para formar una
representación de ciertos aspectos del entorno (por ejemplo, el contenido de una
creencia de que "hay un trozo de chocolate en el escritorio").

#aside[El neurocientífico David Marr (1982) propuso un modelo informacional de
la visión que incluye un componente computacional. En el modelo de Marr, un
organismo procesa la información visual que adquiere en su retina con el fin de
obtener una descripción tridimensional del mundo. En el modelo de Marr, este
proceso ocurre en etapas: primero, a partir de la imagen retinal, se producen
esbozos de las figuras en una escena, luego se reintroduce la textura y se
compone una imagen con información de profundidad centrada en la perspectiva del
observador, y finalmente se recompone la información de profundidad en las
relaciones entre objetos, formando una representación de objetos tridimensional
independiente de la perspectiva del sujeto.

#image("../images/marr.png")

Una idea importante de Marr es que podemos analizar procesos como estos a tres
niveles distintos:

+ El nivel _computacional_: qué es lo que hace el sistema (e.g., producir una
  imagen tridimensional del entorno),
+ El nivel _algorítmico_: qué tipo de representaciones son consumidas o
  producidas por el sistema, y qué procedimientos emplea el sistema para
  manipularlas,
+ El nivel de _implementación_: cómo se realiza el sistema físicamente (e.g.,
  cómo está configurado el sistema retinal, neural y cerebral dedicado a la
  visión).

Una misma tarea puede implementarse mediante la aplicación de distintos
algoritmos, y estos algoritmos pueden realizarse en distintas bases físicas (por
ejemplo, podría implementarse el modelo de Marr en un sistema de visión
computacional).
]

Lo que nos interesa aquí es cómo el modelo representacionalista puede aplicarse
al caso de la acción inteligente, así que vale la pena detenerse un poco en la
manera en que podrían darse explicaciones del comportamiento de acuerdo con este
modelo. En un libro extremadamente influyente y característico del
representacionalismo,#footnote[_The Language of Thought_ (1975).] Jerry Fodor
sugiere que la manera en que actuamos puede describirse de la siguiente manera:

#quote(block: true)[
    [...] el siguiente modelo me parece tremendamente plausible cómo una
    explicación  de la manera en que se decide al menos algunas formas del
    comportamiento:

    + El agente se encuentra en una situación dada ($S$),
    + El agente cree que hay un cierto conjunto de opciones conductuales ($B_1
      ... B_n$)
    + Se predice la consecuencia probable de realizar cada una de esas opciones;
      i.e., el agente computa un conjunto de juicios hipotéticos de una forma
      similar a "si se ejecuta $B_i$ en S, con cierta probabilidad, $C_i$". Qué
      hipotéticos sean computados y qué probabilidades se asignen depende, por
      supuesto, de lo que el organimso sepa o crea sobre situaciones como $S$
      (también dependerá de otras variables que, desde el punto de vista del
      modelo, son meramente ruido: la presión de tiempo, la cantidad de espacio
        computacional disponible para el organismo, etc.)
    + Se le asigna un orden de preferencias a las consecuencias,
    + La decisión conductual del organismo es una función de las preferencias y
      las probabilidades asignadas.
]

Para que este tipo de modelo pueda funcionar, el agente tiene que tener la
capacidad de formar ciertos tipos de representaciones: representaciones de
situaciones, de opciones conductuales, de que tomar ciertas opciones tiene
ciertas consecuencias, etc. Fodor dice:

#quote(block: true)[... de acuerdo a este modelo, tomar decisiones es un proceso
computacional; el acto que el agente realiza es la consecuencia de computaciones
definidas sobre representaciones de acciones posibles. Sin representaciones, no
hay computaciones. Sin computaciones, no hay modelo.]

Esta observación da pie a una versión del argumento típico a favor del
realismo:#footnote[Podemos llamar a los argumentos de este tipo _argumentos de
indispensabilidad_. El más conocido es el de Quine sobre la existencia de las
entidades matemáticas.]

+ Deberíamos comprometernos con la existencia de las entidades postuladas por
  nuestras mejores teorías.
+ El modelo computacional es nuestro mejor modelo de la explicación de la
  acción.
+ El modelo computacional requiere la postulación de representaciones mentales.
+ Deberíamos comprometernos con la existencia de las representaciones mentales.

Por supuesto, aquí todo el trabajo lo está haciendo la segunda premisa. El
soporte que tiene deriva del éxito relativo que la ciencia cognitiva
representacionalista ha tenido en explicar los fenómenos mentales--aquí no me
voy a detener en este punto, pero vale la pena mencionar que el modelo ha sido
altamente popular, y en este sentido, exitoso.

En vistas a lo que vimos en los capítulos anteriores, debería ser claro que un
representacionalismo como el de Fodor se ajusta más al intelectualismo que al
anti-intelectualismo. La acción inteligente requiere que tengamos cierto tipo de
representaciones, cuya posesión es previa al ejercicio inteligente de la acción.
Fodor mismo admite que no siempre tiene que haber una representación explícita
de la información que es relevante para la acción; sin embargo, asume que en
tanto una acción se realiza racionalmente, ha de ir acompañada por un proceso
computacional como el que describe.

== Anti-representacionalismo

No todo el mundo acepta la idea de representación mental. Por ejemplo, Ryle la
rechaza categóricamente. En una carta a Daniel Dennett en referencia a una reseña
que este había escrito sobre _The Language of Thought_, Ryle escribe:

#quote(block: true)[[...] tu reseña hace que me pregunte 1) ¿qué diablos se
supone que sean y hagan esas 'representaciones'? 2) ¿qué significa 'interno'?
[...] Si enumero el alfabeto griego a) en una canción, b) balbuceando, c)
susurrando, d) meramente "en mi cabeza", ¿es solamente d) propiamente algo
'interno'? De modo que cuando balbuceo o entono 'kappa' audiblemente, ¿no es
este ruido una 'representación' de un elemento del alfabeto griego? ([...] Oímos
de 'representaciones de reglas'. ¿Cómo espontáneas o ecos? ¿Rosadas, o roncas?)
O, si tras dictar una y otra vez una regla gramatical o del ajedrez, la
formulación de la regla corre por mi cabeza por puro hábito (como una canción
enloquecedoramente popular), ¿es esa formulación (o cualquier palabra en ella)
una 'representación' de la regla--o de alguna parte de ella (si es que las
reglas tienen partes)?]

No es inesperado que Ryle rechace la propuesta representacionalista de Fodor;
después de todo, como vimos, Fodor explícitamente rechaza las consideraciones de
Ryle en contra de la postulación de entidades internas mentales que jueguen un
rol en la explicación de la acción.

Es importante separar el problema de si el computacionalismo es verdadero o
falso del problema de si el representacionalismo es verdadero o falso. Uno puede
aceptar el representacionalismo sin aceptar el computacionalismo. Por ejemplo,
Putnam (1988) rechaza el computacionalismo porque argumenta que es una hipótesis
demasiado débil: según él, todo sistema abierto instancia a todos los sistemas
computacionales posibles, de modo que decir que un sistema instancia un proceso
computacional no explica su comportamiento (ya que este es al menos compatible
con cualquier proceso computacional).#footnote[Searle (1992) propone un argumento
similar:

#quote(block: true)[Para cualquier programa y para cualquier objeto
suficientemente completo, hay una descripción del objeto según la cual este
implementa el programa. Así, por ejemplo, la muralla que tengo detrás mío en
este momento implementa el programa Wordstar, porque hay un patrón de
mmovimientos que es isomórfico con la estructura formal de Wordstar. Pero si el
muro implementa Wordstar y si es lo suficientemewnte grande, implementa
cualquier programa, incluyendo cualquier programa que implementa el cerebro.
(209)]

] Sin embargo, Putnam acepta la existencia de representaciones mentales.

Para rechazar el representacionalismo, uno debe proveer de un modelo alternativo
que describa y explique los fenómenos que el modelo representacionalista
describe y explica. La dominancia del modelo representacionalista en la ciencia
cognitiva clásica sugiere que esta tarea no es fácil--y que dudas como las de
Ryle, aunque hasta cierto punto razonables, no son suficientes para motivar el
abandono de la noción de representación mental.

Sin embargo, hay maneras de sugerir que la noción de representación mental, como
se la usa en la ciencia cognitiva clásica, es problemática. Ramsey (2007)
observa que el concepto de representación mental a menudo se introduce
analógicamente, en referencia a la existencia de representaciones no-mentales
como signos, señales, diagramas, texto, etc. Sin embargo, que tales cosas
funcionen como representaciones requiere de la existencia de sujetos que puedan
tratarlas como representaciones--sujetos con mentes sofisticadas. ¿Es esto
mismo un requisito para tener representaciones mentales? ¿Pero si es así, no hay
un riesgo de un círculo vicioso, porque para tener representaciones mentales
necesitaríamos tener ciertas capacidades mentales, que deberían explicarse en
términos de la posesión de otras representaciones mentales, que a su vez
deberían explicarse en términos de otras capacidades mentales, que a su vez...?
La noción de representación mental traería en sí misma la posibilidad de caer en
una visión "homuncular" de la mente--en la que las mentes estarían compuestas de
otras mentes, y así sucesivamente.

#question[¿Qué similaridades tiene este argumento a los argumentos Ryleanos
contra el intelectualismo?]

Para responder a este desafío, el representacionalista debe proveer de una
explicación de cómo puede haber representaciones mentales que no dependan de la
existencia de mentes sofisticadas. Dennett mismo ofrece como respuesta a ese
desafío la idea de que las capacidades mentales sofisticadas son el resultado de
la interacción de capacidades más básicas, hasta llegar a un punto en que no es
apropiado decir que esas capacidades son capacidades "mentales". Aunque a un
nivel de análisis dado la mentalidad requiere de comprensión, en su nivel más
básico esto no es así--la falacia homuncular se disuelve porque los sistemas
relevantes simplemente no tienen mentes en el sentido más rico en que los
sistemas que componen las tienen. Junto con una explicación de cómo agentes
complejos pueden ser el resultado de procesos evolutivos, el problema pareciera
resolverse.#footnote[Dennett (2017, c. 8) ofrece una defensa más detallada de la idea.]

Si esta es una manera en que un organismo puede llegar a tener algo así como
representaciones, parece natural suponer que podríamos decir que tener
representaciones _consiste_ en tener ciertos perfiles disposicionales. Esto es
similar a la propuesta Ryleana de que el conocimiento proposicional teórico está
él mismo fundado en la posesión de habilidades. Algunos autores sugieren que
podemos adoptar una noción de representación "tácita" siguiendo esta idea. El
concepto de representaciones tácitas es atractivo porque permite que pensemos en
términos representacionales en casos en los que no es claro que haya
representaciones explícitas. Esto es importante para el debate entre
representacionalistas y anti-representacionalistas porque los últimos han
presentado casos en los que distintos sistemas manifiestan ciertas capacidades
sin tener representaciones explícitas. Por ejemplo, los defensores de la teoría
de sistemas dinámicos (por ejemplo, van Gelder (1995)) han propuesto que no es
necesario implementar sistemas computacionales simbólicamente, y han presentado
varios modelos de sistemas computacionales en los que a primera vista no hay
representaciones explícitas. Quienes defienden la idea de las representaciones
tácitas a menudo sugieren que el que estos sistemas no exhiban representaciones
explícitas no significa que no sean representacionales. El debate continúa.

#question[¿Creen que la noción de representación tácita evita el problema del
circulo vicioso?]

== El formato representacional del saber-cómo

Asumamos, de momento, que algo así como la historia representacionalista es
verdadera respecto al caso del saber cómo. La pregunta que debemos hacernos es:
¿cómo representa la mente la información que se requiere para aplicar el saber
cómo? O puesto de otra manera: si el saber cómo consiste en tener cierto tipo de
representaciones, ¿qué contienen esas representaciones?

Para entender esta pregunta, tenemos que distinguir entre el _vehículo_ de las
representaciones, el _contenido_ de las representaciones y el _formato_ de las
representaciones. Supongan que un basebolista trata de capturar una bola. Si de
hecho supusieron eso, su estado mental ahora contiene una representación de un
basebolista tratando de capturar una bola. Su estado mental es el vehículo de
esa representación, que tiene cierto contenido (aquello que permite que se pueda
tomar a esa representación como una representación del basebolista en vez de
otra cosa). Ahora bien, alguien puede reprentar a un basebolista tratando de
capturar una bola de distintas maneras. Por ejemplo, podrían simplemente tener
en mente una representación _verbal_ del basebolista, o podrían tener una imagen
mental del basebolista. Parte de la manera en que la representación representa
al basebolista es el _formato_ en que lo representa; en un caso, el formato es
verbal, en otro es imaginístico. Decimos que el contenido está codificado de
acuerdo a uno u otro formato.

#question[¿Podría haber alguna diferencia entre la idea de que representaciones
externas, como las pinturas, los mapas, los textos, etc. tienen un formato, y la
idea de que representaciones internas, como los pensamientos, tienen un formato?
¿Debe haber un formato único del pensamiento?]

La conexión entre el contenido de una representación y el formato en el que está
codificado es relativamente arbitraria, en el sentido de que es en general
necesario que un contenido sea codificado de una manera específica. Sin embargo,
distintos formatos pueden permitir la ejecución de distintas tareas
representacionales de distintas maneras, e incluso en los casos en que las
mismas tareas puedan realizarse empleando representaciones en distintos
formatos, puede que sea mejor en cierto sentido emplear representaciones en un
formato específico en vez de otro. Podemos reconocer la cara de alguien usando
una fotografía o una descripción verbal, pero en muchos casos el primer formato
será mucho más eficiente que el segundo. Supongamos, para considerar otro
ejemplo, que queremos producir una imagen de un paisaje en el estilo de Van
Gogh. Podríamos partir de una descripción verbal del paisaje, y del estilo de
Van Gogh, pero es quizás más plausible pensar que recurriríamos a
representaciones pictóricas del paisaje y del estilo. En estos casos, el
contenido sugiere el empleo de cierto formato u otro.#footnote[Cf. Coelho &
Vernazzani (2023).] Una parte importante de la ciencia cognitiva es la
construcción de modelos de la cognición que especifiquen de manera suficiente el
formato y contenido de las representaciones que podrían emplearse para la
realización de ciertas tareas cognitivas (o bien una reconstrucción de los
formatos y contenidos que de hecho empleamos para la realización de ciertas
tareas).#footnote[Aplicando la idea de los niveles de análisis de Marr, este
tipo de preguntas tiene que ver con el nivel algorítmico.]

La idea de que podemos representar un estilo de distintas maneras debería
hacernos pensar en la pregunta acerca del formato representacional del
saber-cómo, precisamente porque un estilo es una manera de hacer algo. La
pregunta: '¿cómo podemos representar un estilo?' no es sino una instancia de la
pregunta '¿cómo podemos representar una manera de actuar?'.

Una manera de abordar la pregunta es, precisamente, tratar de caracterizar las
características de aquello que el saber-cómo representa, y reconstruir esquemas
representacionales que sean capaces de representar algo con esas
características. El problema de caracterizar un _estilo_ es bastante complejo,
así que de momento limitémonos a la pregunta de cómo podemos representar un
procedimiento que tiene un número dado de pasos. Una manera típica de hacer esto
es con una lista de descripciones de las acciones que tienen que seguirse para
completar el procedimiento; esencialmente, esto es lo que tenemos en una
_receta_.#footnote[En la literatura sobre inteligencia artificial este modelo es
importante porque por algún tiempo se pensó que podría construirse sistemas
artificiales que representaran procedimientos a seguir dadas ciertas
condiciones. La representación de la estructura de una situación en la que un
agente puede tomar distintas decisiones o realizar ciertas acciones es llamada
'marco', y la representación de qué hacer en un marco dadas ciertas condiciones
es llamada 'guión'. Cf. Minsky (1974) y Schank & Abelson (1977).]

#question[¿De qué maneras es distinto seguir un estilo y seguir una receta?
Considere el siguiente tipo de caso: un mismo plato puede cocinarse de distintas
maneras en distintas cocinas locales, y dentro de cada cocina local, cada
cocinero puede tener una forma propia de cocinar.]

En su forma más simple, una receta es un esquema que puede emplearse para
conseguir un objetivo; en esto una receta es similar a un _algoritmo_, que
define un procedimiento efectivo para realizar un cálculo (en un sentido amplio
de cálculo). Las recetas de cocina definen un conjunto o lista de ingredientes,
que son elementos nombrados junto con las cantidades que deben usarse, y una
lista de pasos, siguiendo los cuales se supone que obtendremos el resultado
deseado. Por lo general, las recetas asumen que las personas que las usan
entienden a qué cosas y acciones se refieren los términos emplea la receta (por
ejemplo, qué significa 'cardamomo', 'cuchara', 'batir', etc.)

Las representaciones externas de recetas, inscritas en papel y soportes
electrónicos, tienen un formato convencional típico. Ahora bien, estas
representaciones son artefactos, creados por seres humanos para transmitir
información de manera eficiente. En consecuencia, podríamos pensar que el
formato de las representaciones internas de recetas es análogo al formato de
estas representaciones. Después de todo, alguien construyó esas
representaciones, y debe haber tenido representaciones análogas en mente cuando
lo hizo. ¿O no?

#question[Deténganse un segundo y piensen si estas suposiciones son válidas.]

¡No tan rápido! La producción de representaciones externas produce
representaciones en un formato determinado, pero esto no significa que comience
con representaciones en el mismo formato. Si le pedimos a alguien que haga una
pintura de un árbol en invierno, no es necesario que le demos una representación
pictórica de lo que queremos; la descripción verbal es suficiente. Pero en ese
caso hay una transformación de una representación verbal a una pictórica. ¿Por
qué no podría ocurrir algo similar en el paso de la representación interna a la
externa? Si es así, entonces las características del formato interno quedan
sub-determinadas por las características del formato externo, excepto en
términos de ciertos aspectos funcionales (por ejemplo, si el formato externo
permite ciertas transformaciones, es plausible que el formato interno también
las permita).#footnote[Este es el tipo de razonamiento que llevó a Fodor a
postular la idea de que hay algo así como un lenguaje de la mente: dado que el
lenguaje articulado externamente es composicional, Fodor infirió que la mente
debía servirse de un medio que fuera composicional también, y esto fue parte del
caso cumulativo a favor de su hipótesis.]

Esto no significa que los modelos cognitivos de los sistemas representacionales
que están involucrados en el aprendizaje y uso de recetas deban ser
extremadamente distintos al modelo de los sistemas representacionales externos.
Por ejemplo, de acuerdo a ciertas teorías del control motor, la ejecución de
una tarea (como la tarea de seguir los pasos de una receta) requiere una serie
de transformaciones que transforma la intención de los agentes junto con la
información relevante que tengan disponible en órdenes motoras. 'Romper un
huevo' queda representado por una descomposición de sub-tareas motrices y
cognitivas (levantar la mano, acercarla a un huevo, apresar el huevo, levantarlo,
buscar una superficie rígida, mover el huevo contra esa superficie con
suficiente fuerza como para que se rompa la cáscara, alejarlo de esa superficie,
etc.) Una receta, entonces, queda representada por una secuencia de tareas que a
su vez se descomponen en otras secuencias de sub-tareas.#footnote[Cf. Pavese
(2017, 2019).] Esto determina ciertas propiedades estructurales del formato de
las representaciones de procedimientos como recetas, pero no define la manera en
que los componentes de las sub-tareas (que en estos casos tienen que representar
procesos motores) deben quedar representadas.

#question[¿Creen que este modelo es plausible para todas las formas de
saber-cómo? Consideren el caso de tareas no-secuenciales (preparar la cena no
tiene una estructura serial, por ejemplo, ya que las sub-tareas que involucra
pueden ejecutarse en paralelo), y el caso de acciones que no son explícitas
(como tratar de encontrar una prueba para un teorema matemático).]

== ¿Puede los agentes artificiales saber cómo?

Ya vimos cómo es que ciertas formas de representacionalismo parecer tener la
consecuencia de que algunos procesos mentales pueden reproducirse
artificialmente. En verdad, el problema de si la mentalidad artificial es
posible es ortogonal al debate de si hay o no representaciones mentales: en
principio, incluso si no hubiera representaciones mentales, es una cuestión
abierta si podría construirse agentes artificiales que poseyeran una mente
propia. Con los avances recientes en el campo de la inteligencia artificial, el
problema se vuelve cada vez más serio.

El caso del saber cómo es particularmente interesante, porque muchas
aplicaciones de la _inteligencia_ artificial requieren que los agentes
artificiales posean no solo la capacidad de recuperar información, sino en
cierto sentido resolver problemas inteligentemente--manifestando alguna clase
de saber-cómo. La pregunta es importante porque en muchos casos, los sistemas
actuales de inteligencia artificial (como ChatGPT o Gemini) sirven como fuentes
de información para sus usuarios. Si, como vimos en el capítulo 5, puede ser
necesario que las fuentes de saber-cómo tengan ellas mismas saber-cómo, ¿cómo
debemos evaluar el "saber-cómo" que presumiblemente podemos obtener al
interactuar con esos agentes? ¿Y si no son fuentes de saber-cómo directamente,
qué rol pueden tener en la adquisición de saber-cómo? Para atacar estas
preguntas primero debemos considerar el problema general de si podemos
adscribirles saber-cómo. En esta sección veremos algunos argumentos en contra y a
favor de esta posibilidad.

Antes de proceder, hay que hacer algunas aclaraciones. Cuando se discute acerca
de la posibilidad de la inteligencia artificial, hay que distinguir entre:

+ la reproducción artificial de la inteligencia humana
+ la producción de agentes inteligentes artificiales

(Lo primero, obviamente, implica lo segundo). La diferencia radica en el rol que
cumple el modelo de la inteligencia humana. Para quienes están interesados en el
primer objetivo, es importante entender cómo es que los seres humanos poseen
inteligencia, para entonces replicar los mecanismos que permiten que los seres
humanos manifiesten inteligencia. Quienes están interesados meramente en lo
segundo, no es necesario emplear la inteligencia humana como modelo. Lo
importante en este caso es poder construir agentes que tengan ciertas
capacidades y que posean cierto grado de autonomía (claro está, para la
definición de que capacidades y grado de autonomía son relevantes, el mejor
modelo es el de la inteligencia humana; sin embargo, hay que distinguir entre el
modelo humano como definitorio de los estándares de inteligencia, y el modelo
humano como definitorio de qué mecanismos han de implementarse en agentes
artificiales), y no es importante (o al menos, es menos importante) que estos
agentes implementen mecanismos análogos a los que los seres humanos emplean para
manifestar su inteligencia. En la práctica, el desarrollo de agentes
artificiales ha combinado ambas perspectivas de una manera pragmática.

=== El caso en contra

Un argumento clásico en contra de la idea de que los agentes artificiales pueden
tener inteligencia puede aplicarse directamente al caso del saber-cómo.
Recordemos que Ryle distingue entre tener saber-cómo y meramente tener un hábito
de actuar de cierta manera. Lo que distingue al saber-cómo es que es
genuinamente sensible al contexto, no consiste en un mero reproducir un guión de
manera ciega. Algunos defienden la siguiente tesis:

/ Inflexibilidad: Los agentes artificiales solo pueden tener un comportamiento inflexible.

Algunas variaciones de la idea es que los agentes artificiales no son creativos,
carecen de comprensión, no pueden distinguir qué es relevante y qué es
irrelevante de manera flexible en un rango amplio de situaciones,
etc.#footnote[Turing adscribe una versión de este argumento a Lady Lovelace,
quien en sus memorias escribió que 'No hay pretensión de que la Máquina
Analítica _origine_ nada. Puede hacer [solo] lo que nosotros sabemos cómo
ordenarle hacer [_whatever we know how to order it_ to perform]'.] Combinadas
con la idea de que, precisamente, el saber-cómo requiere este tipo de
flexibilidad, la conclusión obvia es que los agentes artificiales no pueden
tener saber-cómo.

Ahora bien, es importante notar que el argumento acepta la posibilidad de que
agentes artificiales realicen ciertas tareas exitosamente. El punto del
argumento es que, incluso si pueden hacer eso, eso no significa que posean
inteligencia o saber-cómo. Recordemos que habitualmente la atribución de
saber-cómo a un sujeto requiere del reconocimiento de que el sujeto posee la
habilidad regular de realizar aquello que supuestamente sabe cómo hacer. Aquí
queda por supuesto que eso es algo que una máquina podría satisfacer, de modo
que el argumento presupone que para la correcta atribución de saber-cómo a un
agente se necesita algo más que eso. Una respuesta al argumento es que esta
suposición es problemática. Quizás, en ciertos contextos, solo basta con que
podamos reconocer las habilidades de los agentes para poder atribuirles saber
cómo. Si empleásemos la heurística del éxito regular para atribuir saber-cómo a
agentes humanos, no hacer lo mismo en el caso de los agentes artificiales sería
una forma de chauvinismo.

#question[Esta sugerencia no es suficiente para mostrar que el argumento es
defectuoso. ¿Qué creen ustedes?]

Por otra parte, el tema es

=== El caso a favor


#set heading(numbering: none, outlined: false)
== Lecturas recomendadas

Hay una vasta literatura sobre la representación mental. Dos libros recientes
importantes son _Representation in Cogntive Science_ (2018) de Nicholas Shea, y
_Representation Reconsidered_ (2007), de William Ramsey.

Vale la pena leer el paper original de Turing sobre la posibilidad de la
inteligencia artificial (1950), donde presentó su famoso _test de la imitación_.

Una buena introducción a la disciplina de la inteligencia artificial es el libro
_Artificial Intelligence: the Basics_ (2012) de Kevin Warwick. Para una visión
más completa de la disciplina, a un nivel más técnico, el texto estándar es
_Artificial Intelligence, A Modern Approach_ (4a ed. 2021), de Peter Norvig y
Stuar Russell. Un texto similar que es accessible en su totalidad en línea es
_Artificial Intelligence: Foundations of Computational Agents_ (2023), de David
Poole y Alan Mackworth (https://artint.info/). 


