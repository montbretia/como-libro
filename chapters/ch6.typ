// vim: spell spelllang=es
#import "../utils.typ": question, aside
#import "../logic.typ": fitch
#set enum(numbering: "1)")

= Ciencia cognitiva y saber cómo

En los capítulos anteriores hemos examinado lo que puede considerarse como el
tema central de las discusiones filosóficas acerca del saber-cómo: la disputa
entre el intelectualismo y el anti-intelectualismo. En este punto deberíamos ser
capaces de tomar posición en esa disputa, o al menos tener una idea acerca de lo
que implicaría adoptar una posición. Sin embargo, esto no agota el tema del
saber cómo; hay más problemas interesantes que vale la pena tratar con cierto
detalle, independientemente de qué postura tome uno respecto al
intelectualismo/anti-intelectualismo. En este capítulo y los siguientes vamos a
ver algunos de ellos.

Primero: sea cual sea nuestra posición respecto al debate entre intelectualistas
y anti-intelectualistas, el saber cómo es un fenómeno en parte psicológico y
cognitivo. Muchos filósofos en la actualidad suscriben a lo que llamaríamos un
_naturalismo mínimo_, que consiste en la idea que la teorización filosófica debe
ser al menos compatible con los métodos y resultados de la ciencia. Una manera
de satisfacer esta condición es requerir además que el trabajo filosófico esté
informado científicamente—esta es una forma más sustantiva de naturalismo. En el
caso del saber cómo, desde una perspectiva naturalista como esta, va a ser
importante examinar qué dicen las ciencias acerca de la naturaleza de los
estados mentales asociados al saber cómo. En este capítulo vamos a examinar
brevemente ciertas líneas de investigación psicológica que son relevantes. El
primer problema que vamos a examinar es cuál podría ser el formato
representacional asociado al saber cómo. Suponiendo que el saber cómo requiere
poseer cierta información acerca de cómo realizar ciertas tareas, ¿cómo son
representadas estas tareas desde una perspectiva representacionalista? Por
supuesto, esto asume que el modelo representacionalista de la mente es correcto.
A partir de esta misma suposición, podríamos inferir que podría haber cierta
clase de agentes artificiales que poseyesen saber cómo—robots con conocimiento
práctico. ¿Es plausible esto?

== Representación mental

No es una exageración decir que la ciencia cognitiva moderna tradicional está
fundada sobre la noción de _representación mental_. En cierto sentido, la noción
general de representación es cotidiana: una pintura puede representar algo, así
como puede hacerlo una palabra, etc. La idea de representación mental es la idea
de que existen ítemes mentales que tienen la propiedad de representar algo. Una
manera de poner la cuestión es que hay estados mentales que son acerca de algo.
Cuando imagino a mi perra Ruby esperándome junto a la puerta, estoy en un estado
mental que apunta a Ruby, es decir, que es acerca de ella. Cuando sumo los
números 2 y 3, estoy en un estado mental acerca de esos números.

De esta observación a menudo se ha pasado a la idea de que nuestros estados
mentales no lo son acerca de algo, sino que "cargan", por así decirlo, con
información acerca de aquello de lo que tratan--es decir, que algunos de
nuestros estados mentales tienen _contenido_. Esto también proviene de la idea
genérica de representación: una pintura, por ejemplo, podría no ser solamente
acerca de algún sujeto, sino además representarlo de una cierta manera. Una
representación representa algo "de una manera" en dos sentidos. Por un lado,
distintas pinturas de un mismo sujeto lo representan de distinta manera porque
tienen distinto contenido. Si veo a mi madre sentada en el sofá desde la
cocina, la veo de distinta manera que cuando la miro desde el comedor; la
diferencia está en el contenido de mi estado mental. En otros casos,
representamos un mismo objeto con un mismo contenido, pero de distintas maneras
en el sentido de que el _modo de representación_ es distinto. Por ejemplo, si
estoy seguro de que mañana no va a llover, tengo un estado mental que representa
la situación posible de que mañana no lloverá. Pero si me pregunto si mañana no
lloverá, también tengo un estado mental que representa esa situación, aunque de
otro modo. Ya vimos esto cuando vimos cómo los intelectualistas acerca del
saber-cómo a veces apelan a la idea de que el saber-cómo requiere un modo
"práctico" de representación, que lo distingue de otros estados mentales con el
mismo contenido proposicional.

Es importante tener en mente que el carácter representacional de las
representaciones, lo que las hace representaciones como tales, es algo
extrínseco, que depende de la existencia de sujetos que puedan reconocerlas como
representaciones. Todo tipo de cosas puede _proveer_ de información que alguien
puede reconocer como acerca de algo. Los grupos de letras que lees ahora
representan palabras, que representan cosas. Para que sirvan ese propósito es
necesario que las reconozcas como representaciones--es necesario que tengas una
manera de _interpretarlas_ (en su sentido más general, tener la capacidad de
interpretar algo como representación consiste en tener la disposición de actuar
de cierta manera cuando uno lo reconoce).

Esto, a su vez, nos da una manera de entender el rol que pueden cumplir las
representaciones mentales. Al reconocer una representación, la interpretamos
realizando una acción. De este modo, las representaciones pueden cumplir el rol
de proveer de información que puede ser empleada luego para guiar la acción.
Vemos que un auto viene por la calle; las representaciones que formamos del auto
nos sirven para _estimar_ la velocidad del vehículo, y la distancia a la que está
de nosotros. En consecuencia, _juzgamos_ que podemos o no tratar de cruzar la
calle. Este ejemplo ilustra además que las acciones que podemos tomar en base a
las representaciones que formamos pueden consistir también en formar otras
representaciones.

Esto da pie a la siguiente imagen sobre la arquitectura de la mente (la manera
en que la mente está organizada funcionalmente). Por un lado, tenemos una
interfaz entre el mundo y la mente, que normalmente consiste en un conjunto de
dispositivos que adquieren información del sujeto mismo y su entorno y forman
representaciones. Luego, tenemos un sistema que procesa esa información de
distintas maneras y produce otras representaciones. Finalmente, hay otra
interfaz entre la mente y el mundo, que consiste en una serie de mecanismos
mediante los cuales el organismo realiza acciones en el mundo. En este modelo,
la mente simplemente es un sistema que transforma información disponible en
información utilizable.

Una versión clásica de esta idea es lo que podemos llamar _la teoría
computacional de la mente_.#footnote[Estrictamente hablando, la teoría
computacional no requiere que tengamos que adoptar el concepto de representación
mental (por ejemplo, ciertos modelos "conexionistas" rechazan la idea de
representación mental--aunque es contencioso si en realidad escapan del modelo
representacionalista), pero sus versiones más tradicionales sí combinan estas
ideas.] De acuerdo a esta, las operaciones mentales consisten en procesos
computacionales; la mente funciona de una manera equivalente a la de una
computadora simbólica.

Una de las maneras más generales de entender cómo funciona una computadora es la
que desarrolló Alan Turing en un paper en #cite(<Turing1936>, form: "year"). Según Turing, es posible
describir cualquier proceso computacional en términos de la operación de un
cierto tipo de máquina extremadamente simple; a estas máquinas se las llama
_máquinas de Turing_. Una máquina de Turing consiste en: 

+ una cinta de longitud arbitrariamente larga, dividida en secciones, en la cual
  pueden escribirse símbolos de un lenguaje determinado,
+ un cabezal lector-escritor con la capacidad de leer y escribir símbolos en la
  cinta en distintas posiciones,
+ un registro del estado de la máquina,
+ una tabla finita de instrucciones tales que, dado el estado de la máquina y el
  símbolo que se lee bajo el cabezal, la máquina:
  + borra el símbolo bajo el cabezal o lo sobreescribe,
  + mueve el cabezal a la izquierda o la derecha, o mantiene el cabezal en la
    posición actual,
  + asume un nuevo estado.

Una máquina de Turing siempre va a tener un estado inicial, y puede llegar a
detenerse (llegar al estado de _parada_).#footnote[Dado un sistema computacional
con un cierto input, uno podría querer saber si la máquina que lo ejecuta va a
llegar eventualmente a detenerse o no. Este problema, conocido como el _problema
de la parada_, no puede resolverse en general; es decir, no hay un algoritmo que
determine, dados un programa y un input, que el programa eventualmente va a
detenerse.] Por ejemplo, supongamos que tenemos una máquina con una cinta
inicialmente vacía con el cabezal en la posición 0:

#figure(kind: "misc", supplement: none)[#table(rows: 1.5em, columns: (1.5em,) * 10, stroke: 0.1pt)]

Asumamos que el lenguaje solo contiene los símbolos "0" y "1". Queremos producir
la cadena de símbolos "1"--es decir, ejecutar un programa que se detenga después
de emitir esa cadena de símbolos. Los inputs del programa son el estado y el
símbolo bajo el lector. Entonces, el programa puede definirse simplemente con la
regla que dice que, dado el estado inicial $sigma$ y cualquier símbolo bajo el
lector, el cabezal debe escribir el símbolo "1", mantener la posición, y pasar
al estado de parada. Tras una aplicación del programa, la máquina queda en la
siguiente configuración (la posición del cabezal está en celeste):

#figure(kind: "misc", supplement: none)[#table(rows: 1.5em, columns: (1.5em,) * 10, stroke: 0.1pt,
table.cell(fill: blue.lighten(50%))[1])]

#question(answer: [Esta máquina escribe una cadena interminable de 1s.])[¿Qué pasaría si reemplazamos esa regla por la regla que dice que, dado
el estado inicial $sigma$ y cualquier símbolo, el cabezal debe escribir el
símbolo "1", moverse a la derecha, y pasar al estado $sigma$ nuevamente?]

Este ejemplo es, quizás, demasiado simple. Imaginemos que quisiéramos producir
la cadena de símbolos "10101010...", con el "10" repitiéndose infinitamente.
¿Qué programa produciría esta cadena de símbolos? En el estado inicial $sigma$,
hacemos que el cabezal escriba "1", nos movemos a la derecha y pasamos al estado
$sigma'$. En el estado $sigma'$, escribimos "0", nos movemos a la derecha, y
pasamos al estado $sigma$ nuevamente.

#figure(caption: [_Algunas iteraciones del programa. La posición del cabezal está
marcada en azul._])[
    #table(
        rows: 1.5em, 
        columns: (1.5em,) * 12, 
        stroke: 0.1pt,
        // row-gutter: 0.2em,
    table.cell(stroke: none)[1], table.cell(fill: blue.lighten(50%))[], [], [], [], [], [], [], [], [], [], table.cell(stroke: none)[],
    table.cell(stroke: none)[2], [1], table.cell(fill: blue.lighten(50%))[], [], [], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none)[3], [1], [0], table.cell(fill: blue.lighten(50%))[], [], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none)[4], [1], [0], [1], table.cell(fill: blue.lighten(50%))[], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none, colspan: 12)[...]
)
]

El modelo de las máquinas de Turing puede generalizarse de maneras interesantes
que permiten sugerir una cierta analogía entre los procesos mentales y los
procesos computacionales que el modelo captura. Por ejemplo, en vez de asumir
que el sistema tiene acceso a una única cinta, es posible proveerlo de "cintas"
para el ingreso de información desde alguna fuente externa que opera
independientemente (del mismo modo que los sentidos de un organismo son
afectados por la operación independientemente de cosas externas al organismo).
La información que ingresa al sistema mediante estas cintas puede después ser
procesada por el sistema. Del mismo modo, puede conectarse el sistema a
distintos mecanismos que produzcan todo tipo de efectos (un sistema de
"salida"), del mismo modo que (asumimos) los procesos mentales de un sujeto
pueden manifestarse en su comportamiento explícito.

Una de las características más notables de este modelo de la operación de la
mente es que permite abstraer de los detalles de la manera en que se realizan
las operaciones. Distintas máquinas de Turing pueden computar lo mismo llevando
a cabo secuencias distintas de operaciones; y además, los detalles de cómo se
realicen esas operaciones no son importantes. Un computador moderno funciona de
una manera muy distinta a cómo habría funcionado la máquina analítica de
Babbage, pero ambos pueden en principio realizar los mismos tipos de
procedimientos computacionales. Y si la caracterización computacional de la
mente es plausible, no importa que las partes de nuestro cuerpo que están a
cargo de la ejecución de procesos computacionales sea distinta a las partes de
una computadora digital, ya que ambos pueden en principio instanciar los mismos
programas. De este modo, el modelo computacional sugiere que al menos cierta
clase de procesos mentales pueden replicarse en la operación de procesos
computacionales artificiales.#footnote[Un paper clásico que defiende esta idea
es "Mind and Machines" (1960), de Hillary Putnam. Putnam posteriomente rechazó
su entusiasmo inicial por el computacionalismo mental; véase por ejemplo su
libro _Representation and Reality_ (1988).]

La noción más general de representación se mezcla bien con el computacionalismo
mental porque amplía el alcance de los tipos de procesos que pueden capturarse
por el modelo. Considérese el caso de la clasificación de objetos en una escena
visual: un sujeto percibe su entorno, y llega a determinar que existen tales o
cuales objetos en este (por ejemplo, ahora juzgo que hay un trozo de chocolate
en mi escritorio en base al contenido de mi experiencia visual). La mente del
sujeto recibe información de su entorno, y lo procesa para formar una
representación de ciertos aspectos del entorno (por ejemplo, el contenido de una
creencia de que "hay un trozo de chocolate en el escritorio").

El neurocientífico David Marr (1982) propuso un modelo informacional de
la visión que incluye un componente computacional. En el modelo de Marr, un
organismo procesa la información visual que adquiere en su retina con el fin de
obtener una descripción tridimensional del mundo. En el modelo de Marr, este
proceso ocurre en etapas: primero, a partir de la imagen retinal, se producen
esbozos de las figuras en una escena, luego se reintroduce la textura y se
compone una imagen con información de profundidad centrada en la perspectiva del
observador, y finalmente se recompone la información de profundidad en las
relaciones entre objetos, formando una representación tridimensional de estos objetos 
independiente de la perspectiva del sujeto.

#figure[
#image("../images/marr.png")
]

Una idea importante de Marr es que podemos analizar procesos como estos a tres
niveles distintos:

+ El nivel _computacional_: qué es lo que hace el sistema (e.g., producir una
  imagen tridimensional del entorno),
+ El nivel _algorítmico_: qué tipo de representaciones son consumidas o
  producidas por el sistema, y qué procedimientos emplea el sistema para
  manipularlas,
+ El nivel de _implementación_: cómo se realiza el sistema físicamente (e.g.,
  cómo está configurado el sistema retinal, neural y cerebral dedicado a la
  visión).

Una misma tarea puede implementarse mediante la aplicación de distintos
algoritmos, y estos algoritmos pueden realizarse en distintas bases físicas (por
ejemplo, podría implementarse el modelo de Marr en un sistema de visión
computacional).

Lo que nos interesa aquí es cómo el modelo representacionalista puede aplicarse
al caso de la acción inteligente, así que vale la pena detenerse un poco en la
manera en que podrían darse explicaciones del comportamiento de acuerdo con este
modelo. En un libro extremadamente influyente y característico del
representacionalismo,#footnote[_The Language of Thought_ (1975).] Jerry Fodor
sugiere que la manera en que actuamos puede describirse de la siguiente manera:

#quote(block: true)[
    [...] el siguiente modelo me parece tremendamente plausible cómo una
    explicación  de la manera en que se decide al menos algunas formas del
    comportamiento:

    + El agente se encuentra en una situación dada ($S$),
    + El agente cree que hay un cierto conjunto de opciones conductuales ($B_1
      ... B_n$)
    + Se predice la consecuencia probable de realizar cada una de esas opciones;
      i.e., el agente computa un conjunto de juicios hipotéticos de una forma
      similar a "si se ejecuta $B_i$ en S, con cierta probabilidad, $C_i$". Qué
      hipotéticos sean computados y qué probabilidades se asignen depende, por
      supuesto, de lo que el organismo sepa o crea sobre situaciones como $S$
      (también dependerá de otras variables que, desde el punto de vista del
      modelo, son meramente ruido: la presión de tiempo, la cantidad de espacio
        computacional disponible para el organismo, etc.)
    + Se le asigna un orden de preferencias a las consecuencias,
    + La decisión del organismo es una función de las preferencias y
      las probabilidades asignadas.
]

Para que este tipo de modelo pueda funcionar, el agente tiene que tener la
capacidad de formar ciertos tipos de representaciones: representaciones de
situaciones, de opciones conductuales, de que tomar ciertas opciones tiene
ciertas consecuencias, etc. Fodor dice:

#quote(block: true)[... de acuerdo a este modelo, tomar decisiones es un proceso
computacional; el acto que el agente realiza es la consecuencia de computaciones
definidas sobre representaciones de acciones posibles. Sin representaciones, no
hay computaciones. Sin computaciones, no hay modelo.]

Esta observación da pie a una versión del argumento típico a favor del
realismo:#footnote[Podemos llamar a los argumentos de este tipo _argumentos de
indispensabilidad_. El más conocido es el de Quine sobre la existencia de las
entidades matemáticas.]

+ Deberíamos comprometernos con la existencia de las entidades postuladas por
  nuestras mejores teorías.
+ El modelo computacional es nuestro mejor modelo de la explicación de la
  acción.
+ El modelo computacional requiere la postulación de representaciones mentales.
+ Deberíamos comprometernos con la existencia de las representaciones mentales.

Por supuesto, aquí todo el trabajo lo está haciendo la segunda premisa. El
soporte que tiene deriva del éxito relativo que la ciencia cognitiva
representacionalista ha tenido en explicar los fenómenos mentales--aquí no me
voy a detener en este punto, pero vale la pena mencionar que el modelo ha sido
altamente popular, y en este sentido, exitoso.

En vistas a lo que vimos en los capítulos anteriores, debería ser claro que un
representacionalismo como el de Fodor se ajusta más al intelectualismo que al
anti-intelectualismo. La acción inteligente requiere que tengamos cierto tipo de
representaciones, cuya posesión es previa al ejercicio inteligente de la acción.
Fodor mismo admite que no siempre tiene que haber una representación explícita
de la información que es relevante para la acción; sin embargo, asume que en
tanto una acción se realiza racionalmente, ha de ir acompañada por un proceso
computacional como el que describe.

== Anti-representacionalismo

No todo el mundo acepta la idea de representación mental. Por ejemplo, Ryle la
rechaza categóricamente. En una carta a Daniel Dennett en referencia a una reseña
que este había escrito sobre _The Language of Thought_, Ryle escribe:

#quote(block: true)[[...] tu reseña hace que me pregunte 1) ¿qué diablos se
supone que sean y hagan esas 'representaciones'? 2) ¿qué significa 'interno'?
[...] Si enumero el alfabeto griego a) en una canción, b) balbuceando, c)
susurrando, d) meramente "en mi cabeza", ¿es solamente d) propiamente algo
'interno'? De modo que cuando balbuceo o entono 'kappa' audiblemente, ¿no es
este ruido una 'representación' de un elemento del alfabeto griego? ([...] Oímos
de 'representaciones de reglas'. ¿Cómo espontáneas o ecos? ¿Rosadas, o roncas?)
O, si tras dictar una y otra vez una regla gramatical o del ajedrez, la
formulación de la regla corre por mi cabeza por puro hábito (como una canción
enloquecedoramente popular), ¿es esa formulación (o cualquier palabra en ella)
una 'representación' de la regla--o de alguna parte de ella (si es que las
reglas tienen partes)?]

No es inesperado que Ryle rechace la propuesta representacionalista de Fodor;
después de todo, como vimos, Fodor explícitamente rechaza las consideraciones de
Ryle en contra de la postulación de entidades internas mentales que jueguen un
rol en la explicación de la acción.

Es importante separar el problema de si el computacionalismo es verdadero o
falso del problema de si el representacionalismo es verdadero o falso. Uno puede
aceptar el representacionalismo sin aceptar el computacionalismo. Por ejemplo,
Putnam (1988) rechaza el computacionalismo porque argumenta que es una hipótesis
demasiado débil: según él, todo sistema abierto instancia a todos los sistemas
computacionales posibles, de modo que decir que un sistema instancia un proceso
computacional no explica su comportamiento (ya que este es al menos compatible
con cualquier proceso computacional).#footnote[Searle (1992) propone un argumento
similar:

#quote(block: true)[Para cualquier programa y para cualquier objeto
suficientemente completo, hay una descripción del objeto según la cual este
implementa el programa. Así, por ejemplo, la muralla que tengo tras de m→ en
este momento implementa el programa Wordstar, porque hay un patrón de
movimientos que es isomórfico con la estructura formal de Wordstar. Pero si el
muro implementa Wordstar y si es lo suficientemente grande, implementa
cualquier programa, incluyendo cualquier programa que implementa el cerebro.
(209)]

] Sin embargo, Putnam acepta la existencia de representaciones mentales.

Para rechazar el representacionalismo, uno debe proveer de un modelo alternativo
que describa y explique los fenómenos que el modelo representacionalista
describe y explica. La dominancia del modelo representacionalista en la ciencia
cognitiva clásica sugiere que esta tarea no es fácil--y que dudas como las de
Ryle, aunque hasta cierto punto razonables, no son suficientes para motivar el
abandono de la noción de representación mental.

Sin embargo, hay maneras de sugerir que la noción de representación mental, como
se la usa en la ciencia cognitiva clásica, es problemática. Ramsey (2007)
observa que el concepto de representación mental a menudo se introduce
analógicamente, en referencia a la existencia de representaciones no-mentales
como signos, señales, diagramas, texto, etc. Sin embargo, que tales cosas
funcionen como representaciones requiere de la existencia de sujetos que puedan
tratarlas como representaciones--sujetos con mentes sofisticadas. ¿Es esto
mismo un requisito para tener representaciones mentales? ¿Pero si es así, no hay
un riesgo de un círculo vicioso, porque para tener representaciones mentales
necesitaríamos tener ciertas capacidades mentales, que deberían explicarse en
términos de la posesión de otras representaciones mentales, que a su vez
deberían explicarse en términos de otras capacidades mentales, que a su vez...?
La noción de representación mental traería en sí misma la posibilidad de caer en
una visión "homuncular" de la mente--en la que las mentes estarían compuestas de
otras mentes, y así sucesivamente.

#question[¿Qué similaridades tiene este argumento a los argumentos Ryleanos
contra el intelectualismo?]

Para responder a este desafío, el representacionalista debe proveer de una
explicación de cómo puede haber representaciones mentales que no dependan de la
existencia de mentes sofisticadas. Dennett mismo ofrece como respuesta a ese
desafío la idea de que las capacidades mentales sofisticadas son el resultado de
la interacción de capacidades más básicas, hasta llegar a un punto en que no es
apropiado decir que esas capacidades son capacidades "mentales". Aunque a un
nivel de análisis dado la mentalidad requiere de comprensión, en su nivel más
básico esto no es así--la falacia homuncular se disuelve porque los sistemas
relevantes simplemente no tienen mentes en el sentido más rico en que los
sistemas que componen las tienen. Junto con una explicación de cómo agentes
complejos pueden ser el resultado de procesos evolutivos, el problema pareciera
resolverse.#footnote[Dennett (2017, c. 8) ofrece una defensa más detallada de la idea.]

Si esta es una manera en que un organismo puede llegar a tener algo así como
representaciones, parece natural suponer que podríamos decir que tener
representaciones _consiste_ en tener ciertos perfiles disposicionales. Esto es
similar a la propuesta Ryleana de que el conocimiento proposicional teórico está
él mismo fundado en la posesión de habilidades. Algunos autores sugieren que
podemos adoptar una noción de representación "tácita" siguiendo esta idea. El
concepto de representaciones tácitas es atractivo porque permite que pensemos en
términos representacionales en casos en los que no es claro que haya
representaciones explícitas. Esto es importante para el debate entre
representacionalistas y anti-representacionalistas porque los últimos han
presentado casos en los que distintos sistemas manifiestan ciertas capacidades
sin tener representaciones explícitas. Por ejemplo, los defensores de la teoría
de sistemas dinámicos (por ejemplo, van Gelder (1995)) han propuesto que no es
necesario implementar sistemas computacionales simbólicamente, y han presentado
varios modelos de sistemas computacionales en los que a primera vista no hay
representaciones explícitas. Quienes defienden la idea de las representaciones
tácitas a menudo sugieren que el que estos sistemas no exhiban representaciones
explícitas no significa que no sean representacionales. El debate continúa.

#question(answer: [No. Que una representación sea tácita no significa que no
tenga que explicarse ella misma en términos de otras representaciones.])[¿Creen
que la noción de representación tácita evita el problema del circulo vicioso?]

== El formato representacional del saber-cómo

Asumamos, de momento, que algo así como la historia representacionalista es
verdadera respecto al caso del saber cómo. La pregunta que debemos hacernos es:
¿cómo representa la mente la información que se requiere para aplicar el saber
cómo? O puesto de otra manera: si el saber cómo consiste en tener cierto tipo de
representaciones, ¿qué contienen esas representaciones?

Para entender esta pregunta, tenemos que distinguir entre el _vehículo_ de las
representaciones, el _contenido_ de las representaciones y el _formato_ de las
representaciones. Supongan que un basebolista trata de capturar una bola. Si de
hecho supusieron eso, su estado mental ahora contiene una representación de un
basebolista tratando de capturar una bola. Su estado mental es el vehículo de
esa representación, que tiene cierto contenido (aquello que permite que se pueda
tomar a esa representación como una representación del basebolista en vez de
otra cosa). Ahora bien, alguien puede reprentar a un basebolista tratando de
capturar una bola de distintas maneras. Por ejemplo, podrían simplemente tener
en mente una representación _verbal_ del basebolista, o podrían tener una imagen
mental del basebolista. Parte de la manera en que la representación representa
al basebolista es el _formato_ en que lo representa; en un caso, el formato es
verbal, en otro es imaginístico. Decimos que el contenido está codificado de
acuerdo a uno u otro formato.

#question[¿Podría haber alguna diferencia entre la idea de que representaciones
externas, como las pinturas, los mapas, los textos, etc. tienen un formato, y la
idea de que representaciones internas, como los pensamientos, tienen un formato?
¿Debe haber un formato único del pensamiento?]

La conexión entre el contenido de una representación y el formato en el que está
codificado es relativamente arbitraria, en el sentido de que es en general
necesario que un contenido sea codificado de una manera específica. Sin embargo,
distintos formatos pueden permitir la ejecución de distintas tareas
representacionales de distintas maneras, e incluso en los casos en que las
mismas tareas puedan realizarse empleando representaciones en distintos
formatos, puede que sea mejor en cierto sentido emplear representaciones en un
formato específico en vez de otro. Podemos reconocer la cara de alguien usando
una fotografía o una descripción verbal, pero en muchos casos el primer formato
será mucho más eficiente que el segundo. Supongamos, para considerar otro
ejemplo, que queremos producir una imagen de un paisaje en el estilo de Van
Gogh. Podríamos partir de una descripción verbal del paisaje, y del estilo de
Van Gogh, pero es quizás más plausible pensar que recurriríamos a
representaciones pictóricas del paisaje y del estilo. En estos casos, el
contenido sugiere el empleo de cierto formato u otro.#footnote[Cf. Coelho &
Vernazzani (2023).] Una parte importante de la ciencia cognitiva es la
construcción de modelos de la cognición que especifiquen de manera suficiente el
formato y contenido de las representaciones que podrían emplearse para la
realización de ciertas tareas cognitivas (o bien una reconstrucción de los
formatos y contenidos que de hecho empleamos para la realización de ciertas
tareas).#footnote[Aplicando la idea de los niveles de análisis de Marr, este
tipo de preguntas tiene que ver con el nivel algorítmico.]

La idea de que podemos representar un estilo de distintas maneras debería
hacernos pensar en la pregunta acerca del formato representacional del
saber-cómo, precisamente porque un estilo es una manera de hacer algo. La
pregunta: '¿cómo podemos representar un estilo?' no es sino una instancia de la
pregunta '¿cómo podemos representar una manera de actuar?'.

Una manera de abordar la pregunta es, precisamente, tratar de caracterizar las
características de aquello que el saber-cómo representa, y reconstruir esquemas
representacionales que sean capaces de representar algo con esas
características. El problema de caracterizar un _estilo_ es bastante complejo,
así que de momento limitémonos a la pregunta de cómo podemos representar un
procedimiento que tiene un número dado de pasos. Una manera típica de hacer esto
es con una lista de descripciones de las acciones que tienen que seguirse para
completar el procedimiento; esencialmente, esto es lo que tenemos en una
_receta_.#footnote[En la literatura sobre inteligencia artificial este modelo es
importante porque por algún tiempo se pensó que podría construirse sistemas
artificiales que representaran procedimientos a seguir dadas ciertas
condiciones. La representación de la estructura de una situación en la que un
agente puede tomar distintas decisiones o realizar ciertas acciones es llamada
'marco', y la representación de qué hacer en un marco dadas ciertas condiciones
es llamada 'guión'. Cf. Minsky (1974) y Schank & Abelson (1977).]

#question[¿De qué maneras es distinto seguir un estilo y seguir una receta?
Considere el siguiente tipo de caso: un mismo plato puede cocinarse de distintas
maneras en distintas cocinas locales, y dentro de cada cocina local, cada
cocinero puede tener una forma propia de cocinar.]

En su forma más simple, una receta es un esquema que puede emplearse para
conseguir un objetivo; en esto una receta es similar a un _algoritmo_, que
define un procedimiento efectivo para realizar un cálculo (en un sentido amplio
de cálculo). Las recetas de cocina definen un conjunto o lista de ingredientes,
que son elementos nombrados junto con las cantidades que deben usarse, y una
lista de pasos, siguiendo los cuales se supone que obtendremos el resultado
deseado. Por lo general, las recetas asumen que las personas que las usan
entienden a qué cosas y acciones se refieren los términos emplea la receta (por
ejemplo, qué significa 'cardamomo', 'cuchara', 'batir', etc.)

Las representaciones externas de recetas, inscritas en papel y soportes
electrónicos, tienen un formato convencional típico. Ahora bien, estas
representaciones son artefactos, creados por seres humanos para transmitir
información de manera eficiente. En consecuencia, podríamos pensar que el
formato de las representaciones internas de recetas es análogo al formato de
estas representaciones. Después de todo, alguien construyó esas
representaciones, y debe haber tenido representaciones análogas en mente cuando
lo hizo. ¿O no?

#question[Deténganse un segundo y piensen si estas suposiciones son válidas.]

¡No tan rápido! La producción de representaciones externas produce
representaciones en un formato determinado, pero esto no significa que comience
con representaciones en el mismo formato. Si le pedimos a alguien que haga una
pintura de un árbol en invierno, no es necesario que le demos una representación
pictórica de lo que queremos; la descripción verbal es suficiente. Pero en ese
caso hay una transformación de una representación verbal a una pictórica. ¿Por
qué no podría ocurrir algo similar en el paso de la representación interna a la
externa? Si es así, entonces las características del formato interno quedan
sub-determinadas por las características del formato externo, excepto en
términos de ciertos aspectos funcionales (por ejemplo, si el formato externo
permite ciertas transformaciones, es plausible que el formato interno también
las permita).#footnote[Este es el tipo de razonamiento que llevó a Fodor a
postular la idea de que hay algo así como un lenguaje de la mente: dado que el
lenguaje articulado externamente es composicional, Fodor infirió que la mente
debía servirse de un medio que fuera composicional también, y esto fue parte del
caso cumulativo a favor de su hipótesis.]

Esto no significa que los modelos cognitivos de los sistemas representacionales
que están involucrados en el aprendizaje y uso de recetas deban ser
extremadamente distintos al modelo de los sistemas representacionales externos.
Por ejemplo, de acuerdo a ciertas teorías del control motor, la ejecución de
una tarea (como la tarea de seguir los pasos de una receta) requiere una serie
de transformaciones que transforma la intención de los agentes junto con la
información relevante que tengan disponible en órdenes motoras. 'Romper un
huevo' queda representado por una descomposición de sub-tareas motrices y
cognitivas (levantar la mano, acercarla a un huevo, apresar el huevo, levantarlo,
buscar una superficie rígida, mover el huevo contra esa superficie con
suficiente fuerza como para que se rompa la cáscara, alejarlo de esa superficie,
etc.) Una receta, entonces, queda representada por una secuencia de tareas que a
su vez se descomponen en otras secuencias de sub-tareas.#footnote[Cf. Pavese
(2017, 2019).] Esto determina ciertas propiedades estructurales del formato de
las representaciones de procedimientos como recetas, pero no define la manera en
que los componentes de las sub-tareas (que en estos casos tienen que representar
procesos motores) deben quedar representadas.

#question[¿Creen que este modelo es plausible para todas las formas de
saber-cómo? Consideren el caso de tareas no-secuenciales (preparar la cena no
tiene una estructura serial, por ejemplo, ya que las sub-tareas que involucra
pueden ejecutarse en paralelo), y el caso de acciones que no son explícitas
(como tratar de encontrar una prueba para un teorema matemático).]

== ¿Pueden los agentes artificiales saber cómo?

Ya vimos cómo es que ciertas formas de representacionalismo parecer tener la
consecuencia de que algunos procesos mentales pueden reproducirse
artificialmente. En verdad, el problema de si la mentalidad artificial es
posible es ortogonal al debate de si hay o no representaciones mentales: en
principio, incluso si no hubiera representaciones mentales, es una cuestión
abierta si podría construirse agentes artificiales que poseyeran una mente
propia. Con los avances recientes en el campo de la inteligencia artificial, el
problema se vuelve cada vez más serio.

El caso del saber cómo es particularmente interesante, porque muchas
aplicaciones de la _inteligencia_ artificial requieren que los agentes
artificiales posean no solo la capacidad de recuperar información, sino en
cierto sentido resolver problemas inteligentemente--manifestando alguna clase
de saber-cómo. La pregunta es importante porque en muchos casos, los sistemas
actuales de inteligencia artificial (como ChatGPT o Gemini) sirven como fuentes
de información para sus usuarios. Si, como vimos en el capítulo 5, puede ser
necesario que las fuentes de saber-cómo tengan ellas mismas saber-cómo, ¿cómo
debemos evaluar el "saber-cómo" que presumiblemente podemos obtener al
interactuar con esos agentes? ¿Y si no son fuentes de saber-cómo directamente,
qué rol pueden tener en la adquisición de saber-cómo? Para atacar estas
preguntas primero debemos considerar el problema general de si podemos
adscribirles saber-cómo. En esta sección veremos algunos argumentos en contra y a
favor de esta posibilidad.

Antes de proceder, hay que hacer algunas aclaraciones. Cuando se discute acerca
de la posibilidad de la inteligencia artificial, hay que distinguir entre:

+ la reproducción artificial de la inteligencia humana
+ la producción de agentes inteligentes artificiales

(Lo primero, obviamente, implica lo segundo). La diferencia radica en el rol que
cumple el modelo de la inteligencia humana. Para quienes están interesados en el
primer objetivo, es importante entender cómo es que los seres humanos poseen
inteligencia, para entonces replicar los mecanismos que permiten que los seres
humanos manifiesten inteligencia. Quienes están interesados meramente en lo
segundo, no es necesario emplear la inteligencia humana como modelo. Lo
importante en este caso es poder construir agentes que tengan ciertas
capacidades y que posean cierto grado de autonomía (claro está, para la
definición de qué capacidades y grado de autonomía son relevantes, el mejor
modelo es el de la inteligencia humana; sin embargo, hay que distinguir entre el
modelo humano como definitorio de los estándares de inteligencia, y el modelo
humano como definitorio de qué mecanismos han de implementarse en agentes
artificiales), y no es importante (o al menos, es menos importante) que estos
agentes implementen mecanismos análogos a los que los seres humanos emplean para
manifestar su inteligencia. En la práctica, el desarrollo de agentes
artificiales ha combinado ambas perspectivas de una manera pragmática.

=== El caso en contra

Un argumento clásico en contra de la idea de que los agentes artificiales pueden
tener inteligencia puede aplicarse directamente al caso del saber-cómo.
Recordemos que Ryle distingue entre tener saber-cómo y meramente tener un hábito
de actuar de cierta manera. Lo que distingue al saber-cómo es que es
genuinamente sensible al contexto, no consiste en un mero reproducir un guion de
manera ciega. Algunos defienden la siguiente tesis:

/ Inflexibilidad: Los agentes artificiales solo pueden tener un comportamiento inflexible.

Algunas variaciones de la idea es que los agentes artificiales no son creativos,
carecen de comprensión, no pueden distinguir qué es relevante y qué es
irrelevante de manera flexible en un rango amplio de situaciones,
etc.#footnote[Turing adscribe una versión de este argumento a Ada Lovelace,
quien en sus memorias escribió que 'no hay pretensión de que la Máquina
Analítica _origine_ nada. Puede hacer [solo] lo que nosotros sabemos cómo
ordenarle hacer [_whatever we know how to order it_ to perform]'. Nótese que
Lovelace sugiere que las máquinas dependen del saber cómo de sus creadores.] Combinadas
con la idea de que, precisamente, el saber-cómo requiere este tipo de
flexibilidad, la conclusión obvia es que los agentes artificiales no pueden
tener saber-cómo.#footnote[
    Formalmente, leyendo $C s$ como 's posee saber cómo', $R s$ como 'la
    conducta de s es sensible al contexto', y $A s$ como 's es un agente
    artificial':
    #fitch(ratio: (1fr, 2fr))[
        + $forall s (C s -> R s)$ (asunción)
        + $forall s (A s -> not R s)$ (asunción)
        + $forall s (A s -> not C s)$ (1, contraposición, 2, silogismo hipotético)
    ]

]

Ahora bien, es importante notar que el argumento acepta la posibilidad de que
agentes artificiales realicen ciertas tareas exitosamente. El punto del
argumento es que, incluso si pueden hacer eso, eso no significa que posean
inteligencia o saber-cómo. Recordemos que habitualmente la atribución de
saber-cómo a un sujeto requiere del reconocimiento de que el sujeto posee la
habilidad regular de realizar aquello que supuestamente sabe cómo hacer. Aquí
queda por supuesto que eso es algo que una máquina podría satisfacer, de modo
que el argumento presupone que para la correcta atribución de saber-cómo a un
agente se necesita algo más que eso. Una respuesta al argumento es que esta
suposición es problemática. Quizás, en ciertos contextos, solo basta con que
podamos reconocer las habilidades de los agentes para poder atribuirles saber
cómo (recuerden la sección 5.5). Si empleásemos la heurística del éxito regular
para atribuir saber-cómo a agentes humanos, no hacer lo mismo en el caso de los
agentes artificiales sería una forma de chauvinismo. De modo que hay razones
para pensar que la premisa sobre la que está fundado el argumento, de que el
saber cómo requiere más que el éxito regular, es problemática.

#question[Esta sugerencia no es suficiente para mostrar que el argumento es
defectuoso. ¿Qué creen ustedes?]

Otra manera de atacar el argumento es rechazar la tesis de la Inflexibilidad.
Algunas maneras 'naive' de defender la tesis ciertamente son dudosas. En
principio, no es necesario que un programa sea inflexible; muchos sistemas
computacionales son capaces de ejecución condicional.#footnote[Una máquina o
sistema es _Turing-completa_ si puede realizar cualquier computación que puede
realizar una máquina de Turing. Todas la máquinas o sistemas Turing-completos
son capaces de ejecución condicional.] En principio, un programa podría definir
todas las respuestas posibles a todas las circunstancias posibles. Por supuesto,
el desarrollo de un programa como ese presenta un desafío técnico importante,
pero la existencia de esta dificultad no implica que tales programas sean
imposibles.

Otra línea de ataque contra la posibilidad del saber-cómo y la inteligencia
artificial es el famoso (o quizás, infame) experimento mental de la pieza china
#cite(<Searle1980>, form: "normal"). El objetivo de Searle era dar un argumento contra la idea de que
se puede explicar la competencia intelectual en términos de meras
transformaciones simbólicas a nivel de la sintaxis (operaciones mecanizables de
transformaciones de símbolos). Esta suposición está a la base del así-llamado
proyecto de la inteligencia artificial 'fuerte', y en particular, es parte de la
justificación para el así-llamado _test de Turing_. El test de Turing es un test
propuesto por Alan Turing (el mismo que desarrolló la idea de las máquinas de
Turing) para evaluar si podía decirse si una máquina poseía inteligencia o
no.#footnote[En realidad, Turing presentó el test como una alternativa a tener
que dar una definición de la inteligencia. En vez de especificar las condiciones
según las cuales algo es inteligente, Turing propuso un criterio implementable
para determinar si algo es inteligente o no.] Esquemáticamente, el test de
Turing consiste en examinar si un evaluador podría ser incapaz de distinguir
entre las respuestas de una máquina y una persona a distintos tipos de
enunciados y preguntas, en el contexto de una conversación.#footnote[Hasta hace
poco, este test era considerado difícil para las máquinas. Con el desarrollo de
las LLMs como ChatGPT, la capacidad de los sistemas computacionales para
mantener conversaciones con personas parece haber avanzado lo suficiente como
para algunos investigadores han propuesto que es necesario desarrollar distintos
tests para la inteligencia artificial. Cf. @JohnsonLaird2023 y @Savage2024.
Véase también @Chollet2019.]

El experimento mental de Searle consiste en lo siguiente:

/ Pieza china: Imaginen a un angloparlante nativo que no sabe nada de chino,
  encerrado en una pieza llana de cajas de símbolos chinos (una base de datos),
  junto a un libro de instrucciones para manipular los símbolos (el programa).
  Imaginen que hay gente que está fuera de la pieza que meten cadenas de
  símbolos chinos (la entrada) a la pieza, que en efecto son preguntas en chino,
  sin que la persona en la pieza sepa que son preguntas. Imaginen que, siguiendo
  las instrucciones dadas en el programa, esta persona es capaz de emitir
  cadenas de símbolos chinos (la salida) al exterior de la pieza, que en efecto
  son respuestas correctas a las preguntas. El programa le permite a la persona
  pasar un test del tipo de Turing sobre la comprensión del chino, sin que en
  realidad la persona entienda una sola palabra de chino.#footnote[Adaptado de
  @Searle1999.]

La aplicación al caso del saber-cómo es más o menos directa: saber chino es una
forma de saber-cómo; implica, al menos en cierto sentido, saber cómo responder a
preguntas formuladas en ese lenguaje. En general, podría haber una pieza similar
para cualquier tarea posible, donde un sujeto que no entiende lo que está
haciendo no obstante es capaz de seguir instrucciones y completar tareas
exitosamente con cierta regularidad. El argumento sería que un sistema capaz de
hacer eso, no por ello sería capaz de ser atribuible con saber-cómo.

Es importante notar que el argumento asume que la tesis de la Inflexibilidad no
es relevante; Searle acepta que el programa podría contener instrucciones para
responder cualquier pregunta en chino, de modo que la flexibilidad del sistema
no está en cuestión.#footnote[Asimismo, las instrucciones del programa podrían
implementar un algoritmo que haría que las respuestas en chino fuesen
'creativas'.] La estructura del argumento, no obstante, es la misma: la
inteligencia requiere de la satisfacción de cierta condición, que las máquinas
intuitivamente no satisfacen. Por lo tanto, las máquinas no pueden ser
inteligentes, o tener saber-cómo.#footnote[Para mayor discusión acerca de la
forma del argumento, véase @Damper2006.]

Hay varias respuestas posibles al argumento de Searle, pero dos de ellas tienen
interés particular para nosotros.

Según la así-llamada _respuesta de sistemas_,
el problema con el experimento mental de Searle es que si bien no es plausible
que el sujeto en la pieza entienda chino, habría que decir que el sistema
completo de la pieza sí entiende chino. La falacia sería decir que para que un
sistema posea saber-cómo, es necesario que una parte específica del sistema debe
tener saber-cómo.#footnote[Recuerden los argumentos de Ryle contra la idea de
que actuar inteligentemente requiere de la realización de un acto inteligente
previo.] La contrarespuesta de Searle a este argumento es proponer una variación
del experimento: podría haber un sujeto que 'internalizara' las operaciones de
la pieza china, memorizando el programa y ejecutándolo tal como lo haría el
sistema de la pieza, pero incluso en este caso sería incorrecto decir que el
sujeto entiende chino.#footnote[Algunos han criticado esta respuesta de Searle
porque creen que es dudoso que alguien pueda internalizar la operación de la
pieza sin llegar a entender chino.]

Según la _respuesta del robot_, el problema del experimento mental de Searle es
que intelectualiza demasiado al entendimiento. Un sistema como el que describe
Searle no es, según quienes defienden esta respuesta, un modelo realista de la
manera en que un agente puede actuar inteligentemente. Para ello, es necesario
entender a los agentes como sujetos integrados de manera causal con el mundo
con el que interactúan, de modo que el sujeto en la pieza está demasiado
desconectado del mundo como para poder tener entendimiento. Es importante notar
que esta respuesta concede a Searle que el sujeto en el experimento mental no
posee entendimiento. No obstante esto, la respuesta dice que eso no muestra que
no pueda haber inteligencia artificial; lo que muestra es que para que pueda
haberla, es necesario diseñar sistemas que estén acoplados de maneras
sustantivas al mundo con el que interactúan. ¿Es esto un requisito para tener
saber-cómo? Searle dice que no. En efecto, Searle sugiere que la incorporación
de maneras de acoplar al sujeto con su entorno es irrelevantes, porque el sujeto
en la pieza de todas maneras carecería de comprensión. Ahora bien, Searle mismo
cree que los agentes artificiales son posibles; su problema es con la idea de
que son posibles de las maneras que hemos descrito hasta
ahora.#footnote[@Searle1984
propone una alternativa, que consiste en pensar que la intencionalidad es una
propiedad no-formal de ciertos sistemas biológicos.]

#question[¿Qué otros argumentos se les ocurren contra el argumento de Searle?
¿Qué respuestas creen que podría dar Searle?]

Otro argumento en contra de la posibilidad de la inteligencia artificial que
puede aplicarse al caso del saber-cómo es el así llamado argumento de la
_informalidad de la inteligencia_. Este argumento está asociado al trabajo del
filósofo Hubert Dreyfus (especialmente su libro _What Computers Can't Do_,
#cite(<Dreyfus1972>, form: "year")), pero fue esbozado primero por @Turing1950.#footnote[@Dreyfus2014 reúne la
mayoría de los trabajos de Dreyfus sobre la naturaleza de las competencias
inteligentes.] La idea central del argumento es
que la posibilidad de la inteligencia artificial requiere que exista cierta
unidad formal de la estructura de normas a las que hay que ser sensible para
actuar inteligentemente, pero que no hay tal cosa, de modo que la inteligencia
artificial no es posible. 


En efecto, el argumento de Dreyfus es que el desarrollo de la inteligencia no
consiste solamente en la adquisición de un conjunto de reglas, sino que
precisamente consiste en la adquisición de habilidades que idealmente son
ejecutadas de manera automática. Según el modelo que propone, la diferencia
entre el experto y el novicio es precisamente que mientras el novicio necesita a
menudo apelar a la consideración explícita de reglas para el reconocimiento de
las situaciones en las que está, la evaluación de las posibles acciones que
puede tomar, y la ejecución de esas acciones, los expertos reconocen las
situaciones en las que están y realizan acciones de una manera que no requiere
de pensamiento explícito:

#quote(block: true)[
  Parece que un novicio hace inferencias usando reglas y hechos del mismo modo
  que lo haría un computador programado heurísticamente, pero que con talento y
  suficiente experiencia, el aprendiz se vuelve un experto que ve intuitivamente
  qué hacer sin aplicar reglas. Por supuesto, una descripción del comportamiento
  habilidoso nunca puede ser tomado como evidencia conclusiva acerca de lo que
  sucede en la mente o el cerebro. Siempre es posible que lo que sucede sea
  algún proceso inconsciente que usa más reglas, y reglas más sofisticadas. Sin
  embargo, nuestra descripción de la adquisición de habilidades va en contra del
  prejuicio tradicional de que la experticia requiere de que se hagan
  inferencias.

  #h(1fr)#cite(<Dreyfus1985>, form: "normal")
]

Puesto de otra manera, el argumento de Dreyfus es que si el proyecto de la
inteligencia artificial concibe el comportamiento inteligente de una manera
sobre-intelectualizada, no puede funcionar. Asume, por tanto, la diferencia
entre estar meramente informado y saber-cómo, y afirma que si bien los sistemas
computacionales pueden estar informados en este sentido, e incluso actuar en
base a la información de la que disponen, no pueden tener saber-cómo en el mismo
sentido del experto.#footnote[Cf. @Dreyfus1986.] Para ello es necesario que
tengan la capacidad de _ver_ relaciones entre la situación en la que están y
situaciones pasadas.

Si bien parte de la crítica de Dreyfus al modelo clásico de la inteligencia
artificial ha sido vindicado con el tiempo, el desarrollo reciente de estas
tecnologías hace dudar del alcance del argumento. En trabajos posteriores,
Dreyfus sugiere que la manera de avanzar con el problema de la inteligencia
artificial y, por tanto del problema del saber-cómo artificial, es abandonar el
cognitivismo y adoptar modelos inspirados en la fenomenología, en particular en
el trabajo de Maurice Merleau-Ponty y Martin Heidegger.#footnote[Cf.
@Dreyfus2004 y @Dreyfus2007.]

@Noe2023 presenta un argumento interesante.#footnote[Véase también @Noe2024.]
Según Noë, la respuesta a la pregunta de si las máquinas pueden pensar es
negativa, porque las máquinas simplemente no _hacen_ nada. Presumiblemente, el
argumento aplica también a la posibilidad del saber-cómo artificial. El criterio
de Nöe es distinto al de los otros autores que hemos visto. Según Noë, lo que es
característico de la agencia es su reflexividad, en un sentido específico. Los
seres humanos actúan porque se ven enfrentados a cosas que requieren oposición o
resistencia. Sin embargo, las máquinas no tienen interés en resolver problemas o
enfrentarse a la realidad de ninguna manera. Parafraseando a John Haugeland
(#cite(<Haugeland1979>, form: "year")) usando un chilenismo, el problema con la inteligencia
artificial es que los computadores no están ni ahí.#footnote[Haugeland dijo:
'the trouble with artificial intelligence is that computers don\'t give a damn.']
Noë sugiere que computadores que pudiesen hacer esto, sí podrían hacer cosas;
pero al hacerlo, dejarían de ser computadores: serían nuestros
equivalentes.#footnote[Noë sugiere además que hay un sentido en que _nuestra_
inteligencia es artificial, porque depende de nuestra inmersión en un entramado
de prácticas que no son de nuestra propia creación, pero que permiten que
podamos actuar reflexivamente.] 

=== El caso a favor

El caso a favor de la posibilidad de la inteligencia artificial y, por tanto, de
la posibilidad de que agentes artificiales posean saber-cómo, parte de las
dificultades que tienen los argumentos en contra de esa posibilidad. No poder
mostrar que algo es imposible da cierto soporte a la idea de que es posible. Sin
embargo, para sostener esa posibilidad es necesario ofrecer más que este
argumento esquemático. Esto es importante porque, en ausencia de argumentos
definitivos a favor o en contra de la posibilidad de la inteligencia artificial,
es racional suspender el juicio, o incluso dudar si la pregunta tiene sentido.

Podemos identificar, a grandes rasgos, dos estrategias para defender la
posibilidad del saber-cómo artificial. La primera estrategia opera desde arriba
hacia abajo (_top-down_), y consiste en derivar esta posibilidad de principios
más generales. La segunda opera desde abajo hacia arriba (_bottom-up_), y
consiste en derivar esta posibilidad de evidencia inductiva.

Los argumentos _top-down_ acerca de la posibilidad de la inteligencia y el
saber-cómo artificiales usualmente parten de la premisa de que la inteligencia
es un fenómeno natural, y que como tal, puede ser explicado de manera
naturalista. El compromiso con el naturalismo es, en este caso, una cuestión
metafísica; la idea (o al menos una versión de la idea) es que la esfera de lo
natural está cerrada bajo ciertas condiciones, que implica que todos los
fenómenos naturales pueden ser explicados en los mismos términos, o que al menos
existe la posibilidad de explicar distintos tipos de fenómenos naturales en los
mismos términos. Esto tiene un componente retrodictivo y otro predictivo: así
como podemos apelar al comportamiento de ciertas cosas dentro de la esfera
natural para explicar el comportamiento de otras cosas dentro de la esfera
natural, también podemos predecir el comportamiento de ciertas cosas dentro de
la esfera natural en términos de otras cosas dentro de esa esfera. Lo crucial es
la analogía. Por ejemplo, es claro que la inteligencia humana tiene, como uno de
sus aspectos, una dimensión de procesamiento de información, y que podemos
construir artefactos que procesen información. En principio, entonces, podría
extenderse la capacidad de los artefactos de procesamiento de información de
modo que alcancen el nivel de los mecanismos de procesamiento de información que
exhiben los seres humanos. Si además hacemos la suposición de que la
inteligencia puede explicarse en términos de ciertas formas de procesamiento de
información, tenemos un argumento para pensar que la inteligencia artificial es
posible.

En efecto, podríamos extender la analogía y decir que, dado que es posible que
la inteligencia aparezca como resultado de procesos naturales (por ejemplo, como
resultado de la evolución biológica), y dado que es posible recrear esos
procesos naturales, es posible que la inteligencia aparezca como resultado de
recrear esos procesos naturales.#footnote[@Dennett2017 esboza otra manera de
conectar la posibilidad de la inteligencia artificial y la evolución.] Este es el pilar fundamental del campo
emergente de la _vida artificial_: en principio, los procesos biológicos
(incluida la inteligencia) pueden ser replicados o sintetizados. Por supuesto,
esto solo eleva la pregunta acerca de la posibilidad de la inteligencia y el
saber-cómo artificial a un nivel más alto: ¿qué razones tenemos para tener
confianza en la hipótesis de que los procesos biológicos son simulables o
sintetizables (y sobre todo, simulables o sintetizables de modo que pueda
emerger la inteligencia como resultado)?

#question[¿Cree que este tipo de argumento es convincente? ¿Puede imaginar otro
argumento _top-down_ acerca de la posibilidad del saber cómo artificial, y qué
desafíos cree que podría haber para defenderlo?]

La otra estrategia, _bottom-up_, consiste en apuntar a los avances de la
tecnología en el campo de la inteligencia artificial y proyectar la posibilidad
de que ciertos agentes artificiales lleguen a poseer saber-cómo. Por ejemplo, es
notable que los LLMs contemporáneos son regularmente capaces de dar respuestas a
preguntas acerca de cómo hacer algo, que es parte de la evidencia de la que nos
servimos para atribuir saber-cómo a las personas. Por supuesto, esta evidencia
no es más decisiva en el caso de los agentes artificiales que en el caso de las
personas--lo que es decir, no es enteramente decisiva. En el caso ordinario, en
que buscamos saber si alguien sabe cómo hacer algo o no, a menudo tratamos de
testear no solamente que estén informados, sino que además su información sea
correcta, y que sean capaces ellos mismos de realizar aquello las tareas en
cuestión. Si un agente artificial pasara sistemáticamente los tests a los que
sometemos a las personas regularmente, no habría en principio problema con decir
que es permisible decir que tienen saber-cómo.#footnote[Un problema con esta
línea argumental es que no considera la posibilidad de falsificar el saber-cómo
(hacer como si uno tiene saber-cómo que realmente no tiene). Quizás sea posible
diseñar agentes artificiales que sin tener saber-cómo puedan falsificarlo
(recuerden, eso sí, que la apuesta del test de Turing es que la capacidad de
simular ciertas clases de comportamiento es equivalente a la posesión de la
capacidad de tener esos comportamientos). Cf.
@Carter2024.]

En su libro _Knowledge and the State of Nature_ (#cite(<Craig1990>, form:
"year")), Edward Craig argumenta que la función del concepto de saber-cómo es la
de poder identificar personas que puedan informarnos acerca de cómo hacer
ciertas cosas, y quizás, para identificar a personas que tengan ciertas
capacidades específicas.#footnote[Craig observa que su hipótesis general, de que
el concepto de conocimiento tiene la función de proveer una manera de
identificar informantes, es problemático en el caso del saber cómo: 'Cuando nos
preguntamos si la señora sabe cómo llegar a la Alcaldía, es cierto, casi siempre
lo que nos preguntamos es si podrá darnos información acerca de cómo llegar a la
Alcaldía. Pero cuando nos atormenta si el niño 'sabe cómo llegar a casa' nos
preocupa si es si puede volver a casa, no si puede dirigirnos allí
eficientemente; claramente puede ser capaz de hacer lo primero sin tener idea de
cómo hacer lo segundo' (p. 150).] Si agentes artificiales fueran capaces de hacer esto,
tendríamos que decir que al menos satisfacen el concepto original de saber-cómo.
Pero quizás sea mejor decir que están a medio camino entre poseer información en
el sentido que un libro posee información, y la manera en que una persona es
capaz de usar esa información para toda clase de propósitos. ¿Qué nos fuerza a
decir que saben cómo hacer algo, en vez de simplemente decir que son una fuente
dinámica de información? Y si poseen un rango más amplio de capacidades, ¿qué
nos fuerza a decir que saben cómo hacer algo en vez de decir que tienen ciertas
capacidades relativas al asunto?

#question[¿No les parece arbitrario dejar la cuestión en estos términos?]

Hemos visto argumentos en contra y a favor de la posibilidad del saber-cómo
artificial. El debate está enteramente abierto. Mucho depende de cómo concibamos
al saber-cómo y la naturaleza de la mente, y sobre esto queda mucho por
investigar aún.

#set heading(numbering: none, outlined: false)
== Lecturas recomendadas

En este capítulo no toco el punto, pero las discusiones en ciencia cognitiva
ofrecen una línea de ataque distinta en los debates sobre la naturaleza del
saber-cómo. Por ejemplo, casos de sujetos con amnesia que mantienen ciertas
competencias motrices parecen sugerir alguna forma de anti-intelectualismo. Sin
embargo, @Stanley2013 presentan un argumento a favor del intelectualismo
basado en esa misma evidencia. Véase @Schwartz2019 para más discusión.

Hay una vasta literatura sobre la representación mental. Dos libros recientes
importantes son _Representation in Cogntive Science_ (2018) de Nicholas Shea, y
_Representation Reconsidered_ (2007), de William Ramsey.

Vale la pena leer el paper original de Turing sobre la posibilidad de la
inteligencia artificial (1950), donde presentó su famoso _test de la imitación_,
y donde responde a una serie de argumentos en contra de la posibilidad de
inteligencias artificiales.

Una buena introducción a la disciplina de la inteligencia artificial es el libro
_Artificial Intelligence: the Basics_ (2012) de Kevin Warwick. Para una visión
más completa de la disciplina, a un nivel más técnico, el texto estándar es
_Artificial Intelligence: A Modern Approach_ (4a ed. 2021), de Peter Norvig y
Stuar Russell. Un texto similar que es accessible en su totalidad en línea es
_Artificial Intelligence: Foundations of Computational Agents_ (2023), de David
Poole y Alan Mackworth (https://artint.info/). 

Una pregunta abierta respecto al rol de la inteligencia artificial como posible
poseedor de saber-cómo o como informante es si puede ser un buen maestro.
@Malfatti2025 examina el problema.
