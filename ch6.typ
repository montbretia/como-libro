#import "utils.typ": question
#set enum(numbering: "1)")

= Ciencia cognitiva y saber cómo

En los capítulos anteriores hemos examinado lo que puede considerarse como el
tema central de las discusiones filosóficas acerca del saber-cómo: la disputa
entre el intelectualismo y el anti-intelectualismo. En este punto deberíamos ser
capaces de tomar posición en esa disputa, o al menos tener una idea acerca de lo
que implicaría adoptar una posición. Sin embargo, esto no agota el tema del
saber cómo; hay más problemas interesantes que vale la pena tratar con cierto
detalle, independientemente de qué postura tome uno respecto al
intelectualismo/anti-intelectualismo. En este capítulo y los siguientes vamos a
ver algunos de ellos.

Primero: sea cual sea nuestra posición respecto al debate entre intelectualistas
y anti-intelectualistas, el saber cómo es un fenómeno en parte psicológico y
cognitivo. Muchos filósofos en la actualidad suscriben a lo que llamaríamos un
_naturalismo mínimo_, que consiste en la idea que la teorización filosófica debe
ser al menos compatible con los métodos y resultados de la ciencia. Una manera
de satisfacer esta condición es requerir además que el trabajo filosófico esté
informado científicamente—esta es una forma más sustantiva de naturalismo. En el
caso del saber cómo, desde una perspectiva naturalista como esta, va a ser
importante examinar qué dicen las ciencias acerca de la naturaleza de los
estados mentales asociados al saber cómo. En este capítulo vamos a examinar
brevemente ciertas líneas de investigación psicológica que son relevantes. El
primer problema que vamos a examinar es cuál podría ser el formato
representacional asociado al saber cómo. Suponiendo que el saber cómo requiere
poseer cierta información acerca de cómo realizar ciertas tareas, ¿cómo son
representadas estas tareas desde una perspectiva representacionalista? Por
supuesto, esto asume que el modelo representacionalista de la mente es correcto.
A partir de esta misma suposición, podríamos inferir que podría haber cierta
clase de agentes artificiales que poseyesen saber cómo—robots con conocimiento
práctico. ¿Es plausible esto?

== Representación mental

No es una exageración decir que la ciencia cognitiva moderna tradicional está
fundada sobre la noción de _representación mental_. En cierto sentido, la noción
general de representación es cotidiana: una pintura puede representar algo, así
como puede hacerlo una palabra, etc. La idea de representación mental es la idea
de que existen ítemes mentales que tienen la propiedad de representar algo. Una
manera de poner la cuestión es que hay estados mentales que son acerca de algo.
Cuando imagino a mi perra Ruby esperándome junto a la puerta, estoy en un estado
mental que apunta a Ruby, es decir, que es acerca de ella. Cuando sumo los
números 2 y 3, estoy en un estado mental acerca de esos números.

De esta observación a menudo se ha pasado a la idea de que nuestros estados
mentales no lo son acerca de algo, sino que "cargan", por así decirlo, con
información acerca de aquello de lo que tratan--es decir, que algunos de
nuestros estados mentales tienen _contenido_. Esto también proviene de la idea
genérica de representación: una pintura, por ejemplo, podría no ser solamente
acerca de algún sujeto, sino además representarlo de una cierta manera. Una
representación representa algo "de una manera" en dos sentidos. Por un lado,
distintas pinturas de un mismo sujeto lo representan de distinta manera porque
tienen distinto contenido. Si veo a mi madre sentada en el sofá desde la
cocina, la veo de distinta manera que cuando la miro desde el comedor; la
diferencia está en el contenido de mi estado mental. En otros casos,
representamos un mismo objeto con un mismo contenido, pero de distintas maneras
en el sentido de que el _modo de representación_ es distinto. Por ejemplo, si
estoy seguro de que mañana no va a llover, tengo un estado mental que representa
la situación posible de que mañana no lloverá. Pero si me pregunto si mañana no
lloverá, también tengo un estado mental que representa esa situación, aunque de
otro modo. Ya vimos esto cuando vimos cómo los intelectualistas acerca del
saber-cómo a veces apelan a la idea de que el saber-cómo requiere un modo
"práctico" de representación, que lo distingue de otros estados mentales con el
mismo contenido proposicional.

Es importante tener en mente que el carácter representacional de las
representaciones, lo que las hace representaciones como tales, es algo
extrínseco, que depende de la existencia de sujetos que puedan reconocerlas como
representaciones. Todo tipo de cosas puede _proveer_ de información que alguien
puede reconocer como acerca de algo. Los grupos de letras que lees ahora
representan palabras, que representan cosas. Para que sirvan ese propósito es
necesario que las reconozcas como representaciones--es necesario que tengas una
manera de _interpretarlas_ (en su sentido más general, tener la capacidad de
interpretar algo como representación consiste en tener la disposición de actuar
de cierta manera cuando uno lo reconoce).

Esto, a su vez, nos da una manera de entender el rol que pueden cumplir las
representaciones mentales. Al reconocer una representación, la interpretamos
realizando una acción. De este modo, las representaciones pueden cumplir el rol
de proveer de información que puede ser empleada luego para guiar la acción.
Vemos que un auto viene por la calle; las representaciones que formamos del auto
nos sirven para _estimar_ la velocidad del vehículo, y la distancia a la que está
de nosotros. En consecuencia, _juzgamos_ que podemos o no tratar de cruzar la
calle. Este ejemplo ilustra además que las acciones que podemos tomar en base a
las representaciones que formamos pueden consistir también en formar otras
representaciones.

Esto da pie a la siguiente imagen sobre la arquitectura de la mente (la manera
en que la mente está organizada funcionalmente). Por un lado, tenemos una
interfaz entre el mundo y la mente, que normalmente consiste en un conjunto de
dispositivos que adquieren información del sujeto mismo y su entorno y forman
representaciones. Luego, tenemos un sistema que procesa esa información de
distintas maneras y produce otras representaciones. Finalmente, hay otra
interfaz entre la mente y el mundo, que consiste en una serie de mecanismos
mediante los cuales el organismo realiza acciones en el mundo. En este modelo,
la mente simplemente es un sistema que transforma información disponible en
información utilizable.

Una versión clásica de esta idea es lo que podemos llamar _la teoría
computacional de la mente_.#footnote[Estrictamente hablando, la teoría
computacional no requiere que tengamos que adoptar el concepto de representación
mental (por ejemplo, ciertos modelos "conexionistas" rechazan la idea de
representación mental--aunque es contencioso si en realidad escapan del modelo
representacionalista), pero sus versiones más tradicionales sí combinan estas
ideas.] De acuerdo a esta, las operaciones mentales consisten en procesos
computacionales; la mente funciona de una manera equivalente a la de una
computadora simbólica.

Una de las maneras más generales de entender cómo funciona una computadora es la
que desarrolló Alan Turing en un paper en 1937. Según Turing, es posible
describir cualquier proceso computacional en términos de la operación de un
cierto tipo de máquina extremadamente simple; a estas máquinas se las llama
_máquinas de Turing_. Una máquina de Turing consiste en: 

+ una cinta de longitud arbitrariamente larga, dividida en secciones, en la cual
  pueden escribirse símbolos de un lenguaje determinado,
+ un cabezal lector-escritor con la capacidad de leer y escribir símbolos en la
  cinta en distintas posiciones,
+ un registro del estado de la máquina,
+ una tabla finita de instrucciones tales que, dado el estado de la máquina y el
  símbolo que se lee bajo el cabezal, la máquina:
  + borra el símbolo bajo el cabezal o lo sobreescribe,
  + mueve el cabezal a la izquierda o la derecha, o mantiene el cabezal en la
    posición actual,
  + asume un nuevo estado.

Una máquina de Turing siempre va a tener un estado inicial, y puede llegar a
detenerse (llegar al estado de _parada_).#footnote[Dado un sistema computacional
con un cierto input, uno podría querer saber si la máquina que lo ejecuta va a
llegar eventualmente a detenerse o no. Este problema, conocido como el _problema
de la parada_, no puede resolverse en general; es decir, no hay un algoritmo que
determine, dados un programa y un input, que el programa eventualmente va a
detenerse.] Por ejemplo, supongamos que tenemos una máquina con una cinta
inicialmente vacía con el cabezal en la posición 0:

#figure[
#table(rows: 1, columns: 10, stroke: 0.1pt)
]

Asumamos que el lenguaje solo contiene los símbolos "0" y "1". Queremos producir
la cadena de símbolos "1"--es decir, ejecutar un programa que se detenga después
de emitir esa cadena de símbolos. Los inputs del programa son el estado y el
símbolo bajo el lector. Entonces, el programa puede definirse simplemente con la
regla que dice que, dado el estado inicial $sigma$ y cualquier símbolo bajo el
lector, el cabezal debe escribir el símbolo "1", mantener la posición, y pasar
al estado de parada.

#question[¿Qué pasaría si reemplazamos esa regla por la regla que dice que, dado
el estado inicial $sigma$ y cualquier símbolo, el cabezal debe escribir el
símbolo "1", moverse a la derecha, y pasar al estado $sigma$ nuevamente?]

El modelo de las máquinas de Turing puede generalizarse de maneras interesantes
que permiten sugerir una cierta analogía entre los procesos mentales y los
procesos computacionales que el modelo captura. Por ejemplo, en vez de asumir
que el sistema tiene acceso a una única cinta, es posible proveerlo de "cintas"
para el ingreso de información desde alguna fuente externa que opera
independientemente (del mismo modo que los sentidos de un organismo son
afectados por la operación independientemente de cosas externas al organismo).
La información que ingresa al sistema mediante estas cintas puede después ser
procesada por el sistema. Del mismo modo, puede conectarse el sistema a
distintos mecanismos que produzcan todo tipo de efectos (un sistema de
"salida"), del mismo modo que (asumimos) los procesos mentales de un sujeto
pueden manifestarse en su comportamiento explícito.

Una de las características más notables de este modelo de la operación de la
mente es que permite abstraer de los detalles de la manera en que se realizan
las operaciones. Distintas máquinas de Turing pueden computar lo mismo llevando
a cabo secuencias distintas de operaciones; y además, los detalles de cómo se
realicen esas operaciones no son importantes. Un computador moderno funciona de
una manera muy distinta a cómo habría funcionado la máquina analítica de
Babbage, pero ambos pueden en principio realizar los mismos tipos de
procedimientos computacionales. Y si la caracterización computacional de la
mente es plausible, no importa que las partes de nuestro cuerpo que están a
cargo de la ejecución de procesos computacionales sea distinta a las partes de
una computadora digital, ya que ambos pueden en principio instanciar los mismos
programas. De este modo, el modelo computacional sugiere que al menos cierta
clase de procesos mentales pueden replicarse en la operación de procesos
computacionales artificiales.#footnote[Un paper clásico que defiende esta idea
es "Mind and Machines" (1960), de Hillary Putnam. Putnam posteriomente rechazó
su entusiasmo inicial por el computacionalismo mental; véáse por ejemplo su
libro _Representation and Reality_ (1988).]

La noción más general de representación se mezcla bien con el computacionalismo
mental porque amplía el alcance de los tipos de procesos que pueden capturarse
por el modelo. Considérese el caso de la clasificación de objetos en una escena
visual: un sujeto percibe su entorno, y llega a determinar que existen tales o
cuáles objetos en este (por ejemplo, ahora juzgo que hay un trozo de chocolate
en mi escritorio en base al contenido de mi experiencia visual). La mente del
sujeto recibe información de su entorno, y lo procesa para formar una
representación de ciertos aspectos del entorno (por ejemplo, el contenido de una
creencia de que "hay un trozo de chocolate en el escritorio").

Lo que nos interesa aquí es cómo el modelo representacionalista puede aplicarse
al caso de la acción inteligente, así que vale la pena detenerse un poco en la
manera en que podrían darse explicaciones del comportamiento de acuerdo con este
modelo. En un libro extremadamente influyente y característico del
representacionalismo,#footnote[_The Language of Thought_ (1975).] Jerry Fodor
sugiere que la manera en que actuamos puede describirse de la siguiente manera:

#quote(block: true)[
    [...] el siguiente modelo me parece tremendamente plausible cómo una
    explicación  de la manera en que se decide al menos algunas formas del
    comportamiento:

    + El agente se encuentra en una situación dada ($S$),
    + El agente cree que hay un cierto conjunto de opciones conductuales ($B_1
      ... B_n$)
    + Se predice la consecuencia probable de realizar cada una de esas opciones;
      i.e., el agente computa un conjunto de juicios hipotéticos de una forma
      similar a "si se ejecuta $B_i$ en S, con cierta probabilidad, $C_i$". Qué
      hipotéticos sean computados y qué probabilidades se asignen depende, por
      supuesto, de lo que el organimso sepa o crea sobre situaciones como $S$
      (también dependerá de otras variables que, desde el punto de vista del
      modelo, son meramente ruido: la presión de tiempo, la cantidad de espacio
        computacional disponible para el organismo, etc.)
    + Se le asigna un orden de preferencias a las consecuencias,
    + La decisión conductual del organismo es una función de las preferencias y
      las probabilidades asignadas.
]

Para que este tipo de modelo pueda funcionar, el agente tiene que tener la
capacidad de formar ciertos tipos de representaciones: representaciones de
situaciones, de opciones conductuales, de que tomar ciertas opciones tiene
ciertas consecuencias, etc. Fodor dice:

#quote(block: true)[... de acuerdo a este modelo, tomar decisiones es un proceso
computacional; el acto que el agente realiza es la consecuencia de computaciones
definidas sobre representaciones de acciones posibles. Sin representaciones, no
hay computaciones. Sin computaciones, no hay modelo.]

Esta observación da pie a una versión del argumento típico a favor del
realismo:#footnote[Podemos llamar a los argumentos de este tipo _argumentos de
indispensabilidad_. El más conocido es el de Quine sobre la existencia de las
entidades matemáticas.]

+ Deberíamos comprometernos con la existencia de las entidades postuladas por
  nuestras mejores teorías.
+ El modelo computacional es nuestro mejor modelo de la explicación de la
  acción.
+ El modelo computacional requiere la postulación de representaciones mentales.
+ Deberíamos comprometernos con la existencia de las representaciones mentales.

Por supuesto, aquí todo el trabajo lo está haciendo la segunda premisa. El
soporte que tiene deriva del éxito relativo que la ciencia cognitiva
representacionalista ha tenido en explicar los fenómenos mentales--aquí no me
voy a detener en este punto, pero vale la pena mencionar que el modelo ha sido
altamente popular, y en este sentido, exitoso.

En vistas a lo que vimos en los capítulos anteriores, debería ser claro que un
representacionalismo como el de Fodor se ajusta más al intelectualismo que al
anti-intelectualismo. La acción inteligente requiere que tengamos cierto tipo de
representaciones, cuya posesión es previa al ejercicio inteligente de la acción.
Fodor mismo admite que no siempre tiene que haber una representación explícita
de la información que es relevante para la acción; sin embargo, asume que en
tanto una acción se realiza racionalmente, ha de ir acompañada por un proceso
computacional como el que describe.

== Anti-representacionalismo

No todo el mundo acepta la idea de representación mental. Por ejemplo, Ryle la
rechaza categóricamente. En una carta a Daniel Dennett en referencia a una reseña
que este había escrito sobre _The Language of Thought_, Ryle escribe:

#quote(block: true)[[...] tu reseña hace que me pregunte 1) ¿qué diablos se
supone que sean y hagan esas 'representaciones'? 2) ¿qué significa 'interno'?
[...] Si enumero el alfabeto griego a) en una canción, b) balbuceando, c)
susurrando, d) meramente "en mi cabeza", ¿es solamente d) propiamente algo
'interno'? De modo que cuando balbuceo o entono 'kappa' audiblemente, ¿no es
este ruido una 'representación' de un elemento del alfabeto griego? ([...] Oímos
de 'representaciones de reglas'. ¿Cómo espontáneas o ecos? ¿Rosadas, o roncas?)
O, si tras dictar una y otra vez una regla gramatical o del ajedrez, la
formulación de la regla corre por mi cabeza por puro hábito (como una canción
enloquecedoramente popular), ¿es esa formulación (o cualquier palabra en ella)
una 'representación' de la regla--o de alguna parte de ella (si es que las
reglas tienen partes)?]

No es inesperado que Ryle rechace la propuesta representacionalista de Fodor;
después de todo, como vimos, Fodor explícitamente rechaza las consideraciones de
Ryle en contra de la postulación de entidades internas mentales que jueguen un
rol en la explicación de la acción.

Es importante separar el problema de si el computacionalismo es verdadero o
falso del problema de si el representacionalismo es verdadero o falso. Uno puede
aceptar el representacionalismo sin aceptar el computacionalismo.

== El formato representacional del saber-cómo

Asumamos, de momento, que algo así como la historia representacionalista es
verdadera respecto al caso del saber cómo. La pregunta que debemos hacernos es:
¿cómo representa la mente la información que se requiere para aplicar el saber
cómo? O puesto de otra manera: si el saber cómo consiste en tener cierto tipo de
representaciones, ¿qué contienen esas representaciones?



== ¿Pueden los robots saber cómo hacer algo?

Ya vimos cómo es que ciertas formas de representacionalismo parecer tener la
consecuencia de que ciertos procesos mentales pueden reproducirse
artificialmente.
