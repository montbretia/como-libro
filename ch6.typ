// vim: spell spelllang=es
#import "utils.typ": question, aside
#set enum(numbering: "1)")

= Ciencia cognitiva y saber cómo

En los capítulos anteriores hemos examinado lo que puede considerarse como el
tema central de las discusiones filosóficas acerca del saber-cómo: la disputa
entre el intelectualismo y el anti-intelectualismo. En este punto deberíamos ser
capaces de tomar posición en esa disputa, o al menos tener una idea acerca de lo
que implicaría adoptar una posición. Sin embargo, esto no agota el tema del
saber cómo; hay más problemas interesantes que vale la pena tratar con cierto
detalle, independientemente de qué postura tome uno respecto al
intelectualismo/anti-intelectualismo. En este capítulo y los siguientes vamos a
ver algunos de ellos.

Primero: sea cual sea nuestra posición respecto al debate entre intelectualistas
y anti-intelectualistas, el saber cómo es un fenómeno en parte psicológico y
cognitivo. Muchos filósofos en la actualidad suscriben a lo que llamaríamos un
_naturalismo mínimo_, que consiste en la idea que la teorización filosófica debe
ser al menos compatible con los métodos y resultados de la ciencia. Una manera
de satisfacer esta condición es requerir además que el trabajo filosófico esté
informado científicamente—esta es una forma más sustantiva de naturalismo. En el
caso del saber cómo, desde una perspectiva naturalista como esta, va a ser
importante examinar qué dicen las ciencias acerca de la naturaleza de los
estados mentales asociados al saber cómo. En este capítulo vamos a examinar
brevemente ciertas líneas de investigación psicológica que son relevantes. El
primer problema que vamos a examinar es cuál podría ser el formato
representacional asociado al saber cómo. Suponiendo que el saber cómo requiere
poseer cierta información acerca de cómo realizar ciertas tareas, ¿cómo son
representadas estas tareas desde una perspectiva representacionalista? Por
supuesto, esto asume que el modelo representacionalista de la mente es correcto.
A partir de esta misma suposición, podríamos inferir que podría haber cierta
clase de agentes artificiales que poseyesen saber cómo—robots con conocimiento
práctico. ¿Es plausible esto?

== Representación mental

No es una exageración decir que la ciencia cognitiva moderna tradicional está
fundada sobre la noción de _representación mental_. En cierto sentido, la noción
general de representación es cotidiana: una pintura puede representar algo, así
como puede hacerlo una palabra, etc. La idea de representación mental es la idea
de que existen ítemes mentales que tienen la propiedad de representar algo. Una
manera de poner la cuestión es que hay estados mentales que son acerca de algo.
Cuando imagino a mi perra Ruby esperándome junto a la puerta, estoy en un estado
mental que apunta a Ruby, es decir, que es acerca de ella. Cuando sumo los
números 2 y 3, estoy en un estado mental acerca de esos números.

De esta observación a menudo se ha pasado a la idea de que nuestros estados
mentales no lo son acerca de algo, sino que "cargan", por así decirlo, con
información acerca de aquello de lo que tratan--es decir, que algunos de
nuestros estados mentales tienen _contenido_. Esto también proviene de la idea
genérica de representación: una pintura, por ejemplo, podría no ser solamente
acerca de algún sujeto, sino además representarlo de una cierta manera. Una
representación representa algo "de una manera" en dos sentidos. Por un lado,
distintas pinturas de un mismo sujeto lo representan de distinta manera porque
tienen distinto contenido. Si veo a mi madre sentada en el sofá desde la
cocina, la veo de distinta manera que cuando la miro desde el comedor; la
diferencia está en el contenido de mi estado mental. En otros casos,
representamos un mismo objeto con un mismo contenido, pero de distintas maneras
en el sentido de que el _modo de representación_ es distinto. Por ejemplo, si
estoy seguro de que mañana no va a llover, tengo un estado mental que representa
la situación posible de que mañana no lloverá. Pero si me pregunto si mañana no
lloverá, también tengo un estado mental que representa esa situación, aunque de
otro modo. Ya vimos esto cuando vimos cómo los intelectualistas acerca del
saber-cómo a veces apelan a la idea de que el saber-cómo requiere un modo
"práctico" de representación, que lo distingue de otros estados mentales con el
mismo contenido proposicional.

Es importante tener en mente que el carácter representacional de las
representaciones, lo que las hace representaciones como tales, es algo
extrínseco, que depende de la existencia de sujetos que puedan reconocerlas como
representaciones. Todo tipo de cosas puede _proveer_ de información que alguien
puede reconocer como acerca de algo. Los grupos de letras que lees ahora
representan palabras, que representan cosas. Para que sirvan ese propósito es
necesario que las reconozcas como representaciones--es necesario que tengas una
manera de _interpretarlas_ (en su sentido más general, tener la capacidad de
interpretar algo como representación consiste en tener la disposición de actuar
de cierta manera cuando uno lo reconoce).

Esto, a su vez, nos da una manera de entender el rol que pueden cumplir las
representaciones mentales. Al reconocer una representación, la interpretamos
realizando una acción. De este modo, las representaciones pueden cumplir el rol
de proveer de información que puede ser empleada luego para guiar la acción.
Vemos que un auto viene por la calle; las representaciones que formamos del auto
nos sirven para _estimar_ la velocidad del vehículo, y la distancia a la que está
de nosotros. En consecuencia, _juzgamos_ que podemos o no tratar de cruzar la
calle. Este ejemplo ilustra además que las acciones que podemos tomar en base a
las representaciones que formamos pueden consistir también en formar otras
representaciones.

Esto da pie a la siguiente imagen sobre la arquitectura de la mente (la manera
en que la mente está organizada funcionalmente). Por un lado, tenemos una
interfaz entre el mundo y la mente, que normalmente consiste en un conjunto de
dispositivos que adquieren información del sujeto mismo y su entorno y forman
representaciones. Luego, tenemos un sistema que procesa esa información de
distintas maneras y produce otras representaciones. Finalmente, hay otra
interfaz entre la mente y el mundo, que consiste en una serie de mecanismos
mediante los cuales el organismo realiza acciones en el mundo. En este modelo,
la mente simplemente es un sistema que transforma información disponible en
información utilizable.

Una versión clásica de esta idea es lo que podemos llamar _la teoría
computacional de la mente_.#footnote[Estrictamente hablando, la teoría
computacional no requiere que tengamos que adoptar el concepto de representación
mental (por ejemplo, ciertos modelos "conexionistas" rechazan la idea de
representación mental--aunque es contencioso si en realidad escapan del modelo
representacionalista), pero sus versiones más tradicionales sí combinan estas
ideas.] De acuerdo a esta, las operaciones mentales consisten en procesos
computacionales; la mente funciona de una manera equivalente a la de una
computadora simbólica.

Una de las maneras más generales de entender cómo funciona una computadora es la
que desarrolló Alan Turing en un paper en 1937. Según Turing, es posible
describir cualquier proceso computacional en términos de la operación de un
cierto tipo de máquina extremadamente simple; a estas máquinas se las llama
_máquinas de Turing_. Una máquina de Turing consiste en: 

+ una cinta de longitud arbitrariamente larga, dividida en secciones, en la cual
  pueden escribirse símbolos de un lenguaje determinado,
+ un cabezal lector-escritor con la capacidad de leer y escribir símbolos en la
  cinta en distintas posiciones,
+ un registro del estado de la máquina,
+ una tabla finita de instrucciones tales que, dado el estado de la máquina y el
  símbolo que se lee bajo el cabezal, la máquina:
  + borra el símbolo bajo el cabezal o lo sobreescribe,
  + mueve el cabezal a la izquierda o la derecha, o mantiene el cabezal en la
    posición actual,
  + asume un nuevo estado.

Una máquina de Turing siempre va a tener un estado inicial, y puede llegar a
detenerse (llegar al estado de _parada_).#footnote[Dado un sistema computacional
con un cierto input, uno podría querer saber si la máquina que lo ejecuta va a
llegar eventualmente a detenerse o no. Este problema, conocido como el _problema
de la parada_, no puede resolverse en general; es decir, no hay un algoritmo que
determine, dados un programa y un input, que el programa eventualmente va a
detenerse.] Por ejemplo, supongamos que tenemos una máquina con una cinta
inicialmente vacía con el cabezal en la posición 0:

#figure(kind: "misc", supplement: none)[#table(rows: 1.5em, columns: (1.5em,) * 10, stroke: 0.1pt)]

Asumamos que el lenguaje solo contiene los símbolos "0" y "1". Queremos producir
la cadena de símbolos "1"--es decir, ejecutar un programa que se detenga después
de emitir esa cadena de símbolos. Los inputs del programa son el estado y el
símbolo bajo el lector. Entonces, el programa puede definirse simplemente con la
regla que dice que, dado el estado inicial $sigma$ y cualquier símbolo bajo el
lector, el cabezal debe escribir el símbolo "1", mantener la posición, y pasar
al estado de parada. Tras una aplicación del programa, la máquina queda en la
siguiente configuración (la posición del cabezal está en rojo):

#figure(kind: "misc", supplement: none)[#table(rows: 1.5em, columns: (1.5em,) * 10, stroke: 0.1pt,
table.cell(fill: red.lighten(50%))[1])]

#question[¿Qué pasaría si reemplazamos esa regla por la regla que dice que, dado
el estado inicial $sigma$ y cualquier símbolo, el cabezal debe escribir el
símbolo "1", moverse a la derecha, y pasar al estado $sigma$ nuevamente?]

Este ejemplo es, quizás, demasiado simple. Imaginemos que quisiéramos producir
la cadena de símbolos "10101010...", con el "10" repitiéndose infinitamente.
¿Qué programa produciría esta cadena de símbolos? En el estado inicial $sigma$,
hacemos que el cabezal escriba "1", nos movemos a la derecha y pasamos al estado
$sigma'$. En el estado $sigma'$, escribimos "0", nos movemos a la derecha, y
pasamos al estado $sigma$ nuevamente.

#figure(caption: [_Algunas iteraciones del programa. La posición del cabezal está
marcada en rojo._])[
    #table(
        rows: 1.5em, 
        columns: (1.5em,) * 12, 
        stroke: 0.1pt,
        // row-gutter: 0.2em,
    table.cell(stroke: none)[1], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [], [], [], [], table.cell(stroke: none)[],
    table.cell(stroke: none)[2], [1], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none)[3], [1], [0], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none)[4], [1], [0], [1], table.cell(fill: red.lighten(50%))[], [], [], [], [], [], [],table.cell(stroke: none)[],
    table.cell(stroke: none, colspan: 12)[...]
)
]

El modelo de las máquinas de Turing puede generalizarse de maneras interesantes
que permiten sugerir una cierta analogía entre los procesos mentales y los
procesos computacionales que el modelo captura. Por ejemplo, en vez de asumir
que el sistema tiene acceso a una única cinta, es posible proveerlo de "cintas"
para el ingreso de información desde alguna fuente externa que opera
independientemente (del mismo modo que los sentidos de un organismo son
afectados por la operación independientemente de cosas externas al organismo).
La información que ingresa al sistema mediante estas cintas puede después ser
procesada por el sistema. Del mismo modo, puede conectarse el sistema a
distintos mecanismos que produzcan todo tipo de efectos (un sistema de
"salida"), del mismo modo que (asumimos) los procesos mentales de un sujeto
pueden manifestarse en su comportamiento explícito.

Una de las características más notables de este modelo de la operación de la
mente es que permite abstraer de los detalles de la manera en que se realizan
las operaciones. Distintas máquinas de Turing pueden computar lo mismo llevando
a cabo secuencias distintas de operaciones; y además, los detalles de cómo se
realicen esas operaciones no son importantes. Un computador moderno funciona de
una manera muy distinta a cómo habría funcionado la máquina analítica de
Babbage, pero ambos pueden en principio realizar los mismos tipos de
procedimientos computacionales. Y si la caracterización computacional de la
mente es plausible, no importa que las partes de nuestro cuerpo que están a
cargo de la ejecución de procesos computacionales sea distinta a las partes de
una computadora digital, ya que ambos pueden en principio instanciar los mismos
programas. De este modo, el modelo computacional sugiere que al menos cierta
clase de procesos mentales pueden replicarse en la operación de procesos
computacionales artificiales.#footnote[Un paper clásico que defiende esta idea
es "Mind and Machines" (1960), de Hillary Putnam. Putnam posteriomente rechazó
su entusiasmo inicial por el computacionalismo mental; véase por ejemplo su
libro _Representation and Reality_ (1988).]

La noción más general de representación se mezcla bien con el computacionalismo
mental porque amplía el alcance de los tipos de procesos que pueden capturarse
por el modelo. Considérese el caso de la clasificación de objetos en una escena
visual: un sujeto percibe su entorno, y llega a determinar que existen tales o
cuáles objetos en este (por ejemplo, ahora juzgo que hay un trozo de chocolate
en mi escritorio en base al contenido de mi experiencia visual). La mente del
sujeto recibe información de su entorno, y lo procesa para formar una
representación de ciertos aspectos del entorno (por ejemplo, el contenido de una
creencia de que "hay un trozo de chocolate en el escritorio").

#aside[El neurocientífico David Marr (1982) propuso un modelo informacional de
la visión que incluye un componente computacional. En el modelo de Marr, un
organismo procesa la información visual que adquiere en su retina con el fin de
obtener una descripción tridimensional del mundo. En el modelo de Marr, este
proceso ocurre en etapas: primero, a partir de la imagen retinal, se producen
esbozos de las figuras en una escena, luego se reintroduce la textura y se
compone una imagen con información de profundidad centrada en la perspectiva del
observador, y finalmente se recompone la información de profundidad en las
relaciones entre objetos, formando una representación de objetos tridimensional
independiente de la perspectiva del sujeto.

#image("images/marr.png")

Una idea importante de Marr es que podemos analizar procesos como estos a tres
niveles distintos:

+ El nivel _computacional_: qué es lo que hace el sistema (e.g., producir una
  imagen tridimensional del entorno),
+ El nivel _algorítmico_: qué tipo de representaciones son consumidas o
  producidas por el sistema, y qué procedimientos emplea el sistema para
  manipularlas,
+ El nivel de _implementación_: cómo se realiza el sistema físicamente (e.g.,
  cómo está configurado el sistema retinal, neural y cerebral dedicado a la
  visión).

Una misma tarea puede implementarse mediante la aplicación de distintos
algoritmos, y estos algoritmos pueden realizarse en distintas bases físicas (por
ejemplo, podría implementarse el modelo de Marr en un sistema de visión
computacional).
]

Lo que nos interesa aquí es cómo el modelo representacionalista puede aplicarse
al caso de la acción inteligente, así que vale la pena detenerse un poco en la
manera en que podrían darse explicaciones del comportamiento de acuerdo con este
modelo. En un libro extremadamente influyente y característico del
representacionalismo,#footnote[_The Language of Thought_ (1975).] Jerry Fodor
sugiere que la manera en que actuamos puede describirse de la siguiente manera:

#quote(block: true)[
    [...] el siguiente modelo me parece tremendamente plausible cómo una
    explicación  de la manera en que se decide al menos algunas formas del
    comportamiento:

    + El agente se encuentra en una situación dada ($S$),
    + El agente cree que hay un cierto conjunto de opciones conductuales ($B_1
      ... B_n$)
    + Se predice la consecuencia probable de realizar cada una de esas opciones;
      i.e., el agente computa un conjunto de juicios hipotéticos de una forma
      similar a "si se ejecuta $B_i$ en S, con cierta probabilidad, $C_i$". Qué
      hipotéticos sean computados y qué probabilidades se asignen depende, por
      supuesto, de lo que el organimso sepa o crea sobre situaciones como $S$
      (también dependerá de otras variables que, desde el punto de vista del
      modelo, son meramente ruido: la presión de tiempo, la cantidad de espacio
        computacional disponible para el organismo, etc.)
    + Se le asigna un orden de preferencias a las consecuencias,
    + La decisión conductual del organismo es una función de las preferencias y
      las probabilidades asignadas.
]

Para que este tipo de modelo pueda funcionar, el agente tiene que tener la
capacidad de formar ciertos tipos de representaciones: representaciones de
situaciones, de opciones conductuales, de que tomar ciertas opciones tiene
ciertas consecuencias, etc. Fodor dice:

#quote(block: true)[... de acuerdo a este modelo, tomar decisiones es un proceso
computacional; el acto que el agente realiza es la consecuencia de computaciones
definidas sobre representaciones de acciones posibles. Sin representaciones, no
hay computaciones. Sin computaciones, no hay modelo.]

Esta observación da pie a una versión del argumento típico a favor del
realismo:#footnote[Podemos llamar a los argumentos de este tipo _argumentos de
indispensabilidad_. El más conocido es el de Quine sobre la existencia de las
entidades matemáticas.]

+ Deberíamos comprometernos con la existencia de las entidades postuladas por
  nuestras mejores teorías.
+ El modelo computacional es nuestro mejor modelo de la explicación de la
  acción.
+ El modelo computacional requiere la postulación de representaciones mentales.
+ Deberíamos comprometernos con la existencia de las representaciones mentales.

Por supuesto, aquí todo el trabajo lo está haciendo la segunda premisa. El
soporte que tiene deriva del éxito relativo que la ciencia cognitiva
representacionalista ha tenido en explicar los fenómenos mentales--aquí no me
voy a detener en este punto, pero vale la pena mencionar que el modelo ha sido
altamente popular, y en este sentido, exitoso.

En vistas a lo que vimos en los capítulos anteriores, debería ser claro que un
representacionalismo como el de Fodor se ajusta más al intelectualismo que al
anti-intelectualismo. La acción inteligente requiere que tengamos cierto tipo de
representaciones, cuya posesión es previa al ejercicio inteligente de la acción.
Fodor mismo admite que no siempre tiene que haber una representación explícita
de la información que es relevante para la acción; sin embargo, asume que en
tanto una acción se realiza racionalmente, ha de ir acompañada por un proceso
computacional como el que describe.

== Anti-representacionalismo

No todo el mundo acepta la idea de representación mental. Por ejemplo, Ryle la
rechaza categóricamente. En una carta a Daniel Dennett en referencia a una reseña
que este había escrito sobre _The Language of Thought_, Ryle escribe:

#quote(block: true)[[...] tu reseña hace que me pregunte 1) ¿qué diablos se
supone que sean y hagan esas 'representaciones'? 2) ¿qué significa 'interno'?
[...] Si enumero el alfabeto griego a) en una canción, b) balbuceando, c)
susurrando, d) meramente "en mi cabeza", ¿es solamente d) propiamente algo
'interno'? De modo que cuando balbuceo o entono 'kappa' audiblemente, ¿no es
este ruido una 'representación' de un elemento del alfabeto griego? ([...] Oímos
de 'representaciones de reglas'. ¿Cómo espontáneas o ecos? ¿Rosadas, o roncas?)
O, si tras dictar una y otra vez una regla gramatical o del ajedrez, la
formulación de la regla corre por mi cabeza por puro hábito (como una canción
enloquecedoramente popular), ¿es esa formulación (o cualquier palabra en ella)
una 'representación' de la regla--o de alguna parte de ella (si es que las
reglas tienen partes)?]

No es inesperado que Ryle rechace la propuesta representacionalista de Fodor;
después de todo, como vimos, Fodor explícitamente rechaza las consideraciones de
Ryle en contra de la postulación de entidades internas mentales que jueguen un
rol en la explicación de la acción.

Es importante separar el problema de si el computacionalismo es verdadero o
falso del problema de si el representacionalismo es verdadero o falso. Uno puede
aceptar el representacionalismo sin aceptar el computacionalismo. Por ejemplo,
Putnam (1988) rechaza el computacionalismo porque argumenta que es una hipótesis
demasiado débil: según él, todo sistema abierto instancia a todos los sistemas
computacionales posibles, de modo que decir que un sistema instancia un proceso
computacional no explica su comportamiento (ya que este es al menos compatible
con cualquier proceso computacional).#footnote[Searle (1992) propone un argumento
similar:

#quote(block: true)[Para cualquier programa y para cualquier objeto
suficientemente completo, hay una descripción del objeto según la cual este
implementa el programa. Así, por ejemplo, la muralla que tengo detrás mío en
este momento implementa el programa Wordstar, porque hay un patrón de
mmovimientos que es isomórfico con la estructura formal de Wordstar. Pero si el
muro implementa Wordstar y si es lo suficientemewnte grande, implementa
cualquier programa, incluyendo cualquier programa que implementa el cerebro.
(209)]

] Sin embargo, Putnam acepta la existencia de representaciones mentales.

Para rechazar el representacionalismo, uno debe proveer de un modelo alternativo
que describa y explique los fenómenos que el modelo representacionalista
describe y explica. La dominancia del modelo representacionalista en la ciencia
cognitiva clásica sugiere que esta tarea no es fácil--y que dudas como las de
Ryle, aunque hasta cierto punto razonables, no son suficientes para motivar el
abandono de la noción de representación mental.

Sin embargo, hay maneras de sugerir que la noción de representación mental, como
se la usa en la ciencia cognitiva clásica, es problemática. Ramsey (2007)
observa que el concepto de representación mental a menudo se introduce
analógicamente, en referencia a la existencia de representaciones no-mentales
como signos, señales, diagramas, texto, etc. Sin embargo, que tales cosas
funcionen como representaciones requiere de la existencia de sujetos que puedan
tratarlas como representaciones--sujetos con mentes sofisticadas. ¿Es esto
mismo un requisito para tener representaciones mentales? ¿Pero si es así, no hay
un riesgo de un círculo vicioso, porque para tener representaciones mentales
necesitaríamos tener ciertas capacidades mentales, que deberían explicarse en
términos de la posesión de otras representaciones mentales, que a su vez
deberían explicarse en términos de otras capacidades mentales, que a su vez...?
La noción de representación mental traería en sí misma la posibilidad de caer en
una visión "homuncular" de la mente--en la que las mentes estarían compuestas de
otras mentes, y así sucesivamente.

#question[¿Qué similaridades tiene este argumento a los argumentos Ryleanos
contra el intelectualismo?]

Para responder a este desafío, el representacionalista debe proveer de una
explicación de cómo puede haber representaciones mentales que no dependan de la
existencia de mentes sofisticadas. Dennett mismo ofrece como respuesta a ese
desafío la idea de que las capacidades mentales sofisticadas son el resultado de
la interacción de capacidades más básicas, hasta llegar a un punto en que no es
apropiado decir que esas capacidades son capacidades "mentales". Aunque a un
nivel de análisis dado la mentalidad requiere de comprensión, en su nivel más
básico esto no es así--la falacia homuncular se disuelve porque los sistemas
relevantes simplemente no tienen mentes en el sentido más rico en que los
sistemas que componen las tienen. Junto con una explicación de cómo agentes
complejos pueden ser el resultado de procesos evolutivos, el problema pareciera
resolverse.#footnote[Dennett (2017, c. 8) ofrece una defensa más detallada de la idea.]

Si esta es una manera en que un organismo puede llegar a tener algo así como
representaciones, parece natural suponer que podríamos decir que tener
representaciones _consiste_ en tener ciertos perfiles disposicionales. Esto es
similar a la propuesta Ryleana de que el conocimiento proposicional teórico está
él mismo fundado en la posesión de habilidades. Algunos autores sugieren que
podemos adoptar una noción de representación "tácita" siguiendo esta idea. El
concepto de representaciones tácitas es atractivo porque permite que pensemos en
términos representacionales en casos en los que no es claro que haya
representaciones explícitas. Esto es importante para el debate entre
representacionalistas y anti-representacionalistas porque los últimos han
presentado casos en los que distintos sistemas manifiestan ciertas capacidades
sin tener representaciones explícitas. Por ejemplo, los defensores de la teoría
de sistemas dinámicos (por ejemplo, van Gelder (1995)) han propuesto que no es
necesario implementar sistemas computacionales simbólicamente, y han presentado
varios modelos de sistemas computacionales en los que a primera vista no hay
representaciones explícitas. Quienes defienden la idea de las representaciones
tácitas a menudo sugieren que el que estos sistemas no exhiban representaciones
explícitas no significa que no sean representacionales. El debate continúa.

#question[¿Creen que la noción de representación tácita evita el problema del
circulo vicioso?]

== El formato representacional del saber-cómo

Asumamos, de momento, que algo así como la historia representacionalista es
verdadera respecto al caso del saber cómo. La pregunta que debemos hacernos es:
¿cómo representa la mente la información que se requiere para aplicar el saber
cómo? O puesto de otra manera: si el saber cómo consiste en tener cierto tipo de
representaciones, ¿qué contienen esas representaciones?

== ¿Puede haber agentes artificiales que sepan cómo hacer algo?

Ya vimos cómo es que ciertas formas de representacionalismo parecer tener la
consecuencia de que algunos procesos mentales pueden reproducirse
artificialmente. En verdad, el problema de si la mentalidad artificial es
posible es ortogonal al debate de si hay o no representaciones mentales: en
principio, incluso si no hubiera representaciones mentales, es una cuestión
abierta si podría construirse agentes artificiales que poseyeran una mente
propia. Con los avances recientes en el campo de la inteligencia artificial, el
problema se vuelve cada vez más serio.

El caso del saber cómo es particularmente interesante, porque muchas
aplicaciones de la _inteligencia_ artificial requieren que los agentes
artificiales posean no solo la capacidad de recuperar información, sino en
cierto sentido resolver problemas inteligentemente--manifiestando alguna clase
de saber-cómo. La pregunta es importante porque en muchos casos, los sistemas
actuales de inteligencia artificial (como ChatGPT o Gemini) sirven como fuentes
de información para sus usuarios. Si, como vimos en el capítulo 5, puede ser
necesario que las fuentes de saber-cómo tengan ellas mismas saber-cómo, ¿cómo
debemos evaluar el "saber-cómo" que presumiblemente podemos obtener al
interactuar con esos agentes? ¿Y si no son fuentes de saber-cómo directamente,
qué rol pueden tener en la adquisición de saber-cómo? Para atacar estas
preguntas primero debemos considerar el problema general de si podemos
adscribirles saber-cómo. En esta sección veremos algunos argumentos en contra y a
favor de esta posibilidad.

=== El argumento en contra



=== El argumento a favor


#set heading(numbering: none, outlined: false)
== Lecturas recomendadas

Hay una vasta literatura sobre la representación mental. Dos libros recientes
importantes son _Representation in Cogntive Science_ (2018) de Nicholas Shea, y
_Representation Reconsidered_ (2007), de William Ramsey.

Vale la pena leer el paper originales de Turing sobre la posibilidad de la
inteligencia artificial (1950), donde presentó su famoso _test de la imitación_.

Una buena introducción a la disciplina de la inteligencia artificial es el libro
_Artificial Intelligence: the Basics_ (2012) de Kevin Warwick. Para una visión
más completa de la disciplina, a un nivel más técnico, el texto estándar es
_Artificial Intelligence, A Modern Approach_ (4a ed. 2021), de Peter Norvig y
Stuar Russell. Un texto similar accessible en su totalidad en línea es
_Artificial Intelligence: Foundations of Computational Agents_ (2023), de David
Poole y Alan Mackworth (https://artint.info/). 


